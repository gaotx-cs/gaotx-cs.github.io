{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Homework 7: Recurrent Neural Networks\n",
        "\n",
        "In this homework, you will implement a simple Recurrent Neural Network (RNN) that processes an input sequence $x=\\{x_1, \\cdots, x_T\\}$ to generate an output sequence $y=\\{y_1, \\cdots, y_T\\}$. The tasks includes\n",
        "- Tanh and softmax\n",
        "- Forward propogation\n",
        "- Backpropogation through time\n",
        "- RNN class"
      ],
      "metadata": {
        "id": "jqVj_MwtQVUM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0 - Import Libraries and Tanh [1/1]\n",
        "\n",
        "In this assingment, we only use `numpy` to implement RNN and `matplotlib` for plot."
      ],
      "metadata": {
        "id": "fDCSeKZcQJs2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASbkW0aWpzXH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "np.random.seed(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As discussed in the lectures, the Tanh activation function is commonly used in Natural Language Processing (NLP). Here, youâ€™ll define a class `Tanh` with two methods: `forward()` for activation and `derivative()` for computing gradients during backpropagation.\n",
        "\n",
        "$$\n",
        "\\tanh(x) = \\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}\n",
        "$$\n",
        "\n",
        "**Exercise [0.5/0.5]**: Implement class `Tanh`."
      ],
      "metadata": {
        "id": "ELBSbul6TKDM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Tanh:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward(self, x):\n",
        "        ### Code Here ###\n",
        "\n",
        "        ### Code Here ###\n",
        "\n",
        "    def derivative(self, x):\n",
        "        ### Code Here ###\n",
        "\n",
        "        ### Code Here ###\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return self.forward(x)"
      ],
      "metadata": {
        "id": "CzqPYQLR8Xhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Test Tanh\n",
        "act = Tanh()\n",
        "x = np.random.randn(2,3)\n",
        "print(f\"act(x) =\\n {act(x)}\")\n",
        "print(f\"act.derivative(x) =\\n {act.derivative(x)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztARRX9vVFRF",
        "outputId": "fc960cc4-03bc-4011-ec70-992470e7a717"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "act(x) =\n",
            " [[ 0.92525207 -0.5453623  -0.48398233]\n",
            " [-0.79057703  0.69903334 -0.98015695]]\n",
            "act.derivative(x) =\n",
            " [[0.1439086  0.70257996 0.76576111]\n",
            " [0.37498795 0.51135239 0.03929236]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Additionally, we will define the Softmax activation to compute the output $\\hat{y}$.\n",
        "\n",
        "Note: The input $x$ has shape `(batch_size, input_size)`. Your implementation should ensure the output `y = softmax(x)` retains the shape `(batch_size, input_size)`, and the sum of `y` across each sample should be `1`.\n",
        "\n",
        "**Exercise [0.5/0.5]**: Implement softmax."
      ],
      "metadata": {
        "id": "_dDpsfG1TsJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "    ### Code Here ###\n",
        "\n",
        "    ### Code Here ###"
      ],
      "metadata": {
        "id": "rjxVXLXXTvfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Test softmax\n",
        "x = np.random.randn(4,5)\n",
        "y = softmax(x)\n",
        "print(f\"softmax(x) =\\n {y}\")\n",
        "print(f\"sum of y for each sample =\\n {np.sum(y, axis=1, keepdims=True)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57lH_uHIU6Wo",
        "outputId": "714c3639-6ae2-40e9-c07a-8caae481b3be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "softmax(x) =\n",
            " [[0.18376865 0.4103551  0.03269533 0.28088273 0.09229819]\n",
            " [0.17606771 0.11032588 0.11556621 0.08852581 0.50951439]\n",
            " [0.39110427 0.27540117 0.05782917 0.19338811 0.08227728]\n",
            " [0.10790932 0.17883443 0.21565153 0.24922814 0.24837658]]\n",
            "sum of y for each sample =\n",
            " [[1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1 - Implement Recurrent Neural Network\n"
      ],
      "metadata": {
        "id": "DulWZP4dqkTE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Initialization [1/1]\n",
        "\n",
        "Recall that we initialize the weights and bias in MLP using random initializaiton. Here we do the same random initializaiton for RNN. Specifically, for a weight matrix $W\\in \\mathbb{R}^{m\\times n}$, each entry is randomly initialized using an i.i.d. Gaussian distribution:\n",
        "$$\n",
        "W_{ij}\\sim N\\left(0,\\frac{2}{n+m}\\right)\n",
        "$$\n",
        "We are fine to use zero initializaiton for bias. Note that in RNN, we have three weight matrices shared across all time steps: $W_x\\in \\mathbb{R}^{n_h\\times n_x}$, $W_y\\in \\mathbb{R}^{n_y\\times n_h}$, and $W_h\\in \\mathbb{R}^{n_h \\times n_h}$."
      ],
      "metadata": {
        "id": "uV-NcrFxzzLc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise [2/2]**: Implement `initialize_rnn_parameters` to randomly initilaize weights `Wx`, `Wy`, `Wh`, and biases `bh` and `by`. Return a dictionary `parameters` that contains all parameters."
      ],
      "metadata": {
        "id": "IFWUaHn1-eU6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize weights and biases\n",
        "def initialize_rnn_parameters(n_x, n_h, n_y):\n",
        "    parameters = {}\n",
        "    ### Code Here ###\n",
        "\n",
        "    ### Code Here ###\n",
        "\n",
        "    parameters['Wx'] = Wx\n",
        "    parameters['Wh'] = Wh\n",
        "    parameters['Wy'] = Wy\n",
        "    parameters['bh'] = bh\n",
        "    parameters['by'] = by\n",
        "    return parameters"
      ],
      "metadata": {
        "id": "mLRR4B57-gKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test initialize_rnn_parameters\n",
        "np.random.seed(1)\n",
        "n_x = 3\n",
        "n_h = 9\n",
        "n_y = 5\n",
        "parameters = initialize_rnn_parameters(n_x, n_h, n_y)\n",
        "print(\"Wx:\", parameters['Wx'])\n",
        "print(\"Wh:\", parameters['Wh'])\n",
        "print(\"Wy:\", parameters['Wy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGbdkf-k_qXc",
        "outputId": "fc0eefec-c642-4319-ad5e-9890609f634f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wx: [[ 0.66313622 -0.24974851 -0.21562521]\n",
            " [-0.43803761  0.35330119 -0.93959924]\n",
            " [ 0.71231642 -0.31076142  0.13024717]\n",
            " [-0.10180503  0.59690307 -0.84104892]\n",
            " [-0.13162627 -0.15678953  0.46285944]\n",
            " [-0.44902873 -0.07039352 -0.3583842 ]\n",
            " [ 0.01723369  0.23793331 -0.4493259 ]\n",
            " [ 0.4673315   0.36807287  0.20514245]\n",
            " [ 0.3677729  -0.27913073 -0.05016972]]\n",
            "Wh: [[-0.31192314 -0.08929603  0.17678516 -0.23055358 -0.13225118 -0.22905757\n",
            "  -0.28173521 -0.22374871 -0.00422153]\n",
            " [-0.37243678  0.07813857  0.55326739  0.24734805 -0.06394518 -0.29587632\n",
            "  -0.24905276  0.56415153  0.01693592]\n",
            " [-0.21233188  0.06363849  0.70008505  0.04005298  0.20573437  0.10005677\n",
            "  -0.11741662 -0.3808394  -0.11644757]\n",
            " [-0.06963141  0.19554106  0.27966114  0.31036736  0.09519578  0.29504705\n",
            "  -0.25146598  0.41762272  0.17097661]\n",
            " [-0.09936428  0.16283938 -0.02519057  0.3772098   0.50660561  0.72852514\n",
            "  -0.46549878 -0.48137127 -0.16815529]\n",
            " [ 0.05334569  0.29205631  0.10521165 -0.67406707 -0.102068    0.27599155\n",
            "   0.07669825  0.25400373 -0.07410938]\n",
            " [-0.06691936  0.06218713  0.13668388  0.06609991  0.03966955 -0.2235541\n",
            "   0.1258546   0.04060709  0.37649464]\n",
            " [ 0.39963929  0.06171881 -0.12509498 -0.21291014  0.14116478  0.02578002\n",
            "  -0.11461789  0.01453229 -0.20666695]\n",
            " [ 0.23267734 -0.14904285  0.40816923  0.13449721  0.19785951 -0.36497062\n",
            "   0.05646081  0.24685215 -0.3179002 ]]\n",
            "Wy: [[-0.70434796  0.08628998 -3.63292695  0.83383337  2.23873064 -2.27406543\n",
            "   0.92745748 -3.47197556 -0.10237869]\n",
            " [-4.27493183  2.96699237  1.08184913 -0.06513034 -2.05088487  3.37004142\n",
            "   5.20446203 -4.91575795  3.2705826 ]\n",
            " [ 4.30635911  0.89429489 -3.17296497  2.28419701 -0.47867013 -1.59782379\n",
            "  -3.25442793  1.4565853   2.0975698 ]\n",
            " [-1.64970725  1.37731553 -3.02764273  2.12152488  0.12320549 -0.49361722\n",
            "  -0.26919428  2.29885669  1.98540258]\n",
            " [ 1.40083358  0.36432316  0.20589535  1.63608039  0.61512278  1.80586128\n",
            "  -0.82049186 -6.44197521  2.74847155]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-2 RNN Layer [2/2]\n",
        "\n",
        "implement RNN. Recall that give a sequence $\\{x_1, \\dots, x_T\\}$, a RNN unit take a hidden state vector $h_{t-1}$ from previous layer and the current input $x_t$ from the sequence, then update the hidden state\n",
        "$$\n",
        "h_t = \\tanh(W_h h_{t-1} + W_x x_t + b_h)\n",
        "$$\n",
        "Then this update hidden state is used to compute the output at time $t$:\n",
        "$$\n",
        "\\hat{y}_t = \\text{softmax}(W_y h_t + b_y)\n",
        "$$\n",
        "\n",
        "However, to facility backpropogation, we will introduce extra intermediate variables and their values will be stored in the `cache` that will be used in backpropogation to compute the gradients.\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "&z_t = W_h h_{t-1} + W_x x_t + b_h\\\\\n",
        "&h_t = \\phi(z_t)\\\\\n",
        "&u_t = W_y h_t + b_y\\\\\n",
        "&\\hat{y}_t = \\text{softmax}(u_t)\n",
        "\\end{align}\n",
        "$$\n",
        "Note that we will not store the values of $u_t$, since it does not explicitly use thanks to the combination of cross entropy loss and softmax. Thus, we only need to return `yhat_t`, `h_t`, and `z_t`.\n",
        "\n",
        "**Exercise [2/2]**: Implement one layer RNN `rnn_layer` with input `x_t` and `h_prev`, and `parameters`. Return the `yhat_t`, `h_t`, and `z_t`"
      ],
      "metadata": {
        "id": "aTIOPPqE_3k3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rnn_layer(x_t, h_prev, parameters, act=Tanh()):\n",
        "    # xt: input at time t, shape (batch_size, input_size)\n",
        "    # h_prev: previous hidden state, shape (batch_size, hidden_size)\n",
        "    # Wh: weights for hidden to hidden, shape (hidden_size, hidden_size)\n",
        "    # Wx: weights for input to hidden, shape (hidden_size, input_size)\n",
        "    # Wy: weights for hidden to output, shape (output_size, hidden_size)\n",
        "    # bh: bias for hidden state, shape (hidden_size, 1)\n",
        "    # by: bias for output, shape (output_size, 1)\n",
        "\n",
        "    Wh = parameters['Wh']\n",
        "    Wx = parameters['Wx']\n",
        "    Wy = parameters['Wy']\n",
        "    bh = parameters['bh']\n",
        "    by = parameters['by']\n",
        "\n",
        "    ### Code Here ###\n",
        "\n",
        "    ### Code Here ###\n",
        "\n",
        "    return yhat_t, h_t, z_t"
      ],
      "metadata": {
        "id": "fukEn0qFqtp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test rnn_layer()\n",
        "np.random.seed(1)\n",
        "batch_size = 2\n",
        "x = np.random.randn(batch_size, n_x)\n",
        "pre_h = np.random.randn(batch_size, n_h)\n",
        "\n",
        "y_t, h_t, z_t = rnn_layer(x, pre_h, parameters)\n",
        "print(\"y_t.shape:\", y_t.shape)\n",
        "print(\"y_t:\", y_t)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0V6ZFvht-Uk",
        "outputId": "f1d3590b-33b7-4d41-c8eb-44417d22f55a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_t.shape: (2, 5)\n",
            "y_t: [[2.31497458e-04 9.24265539e-03 9.75012681e-01 1.44409055e-02\n",
            "  1.07226113e-03]\n",
            " [1.08739880e-05 9.99969433e-01 1.37498024e-08 4.22887287e-06\n",
            "  1.54503763e-05]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-3 RNN Forward Pass [2/2]\n",
        "\n",
        "Now, we are ready to build the `forward()` pass for RNN. Given `inputs`, we stack the `rnn_layer` w.r.t. the `sequence_length`. We store all the intermediate values `h_t`, `yhat_t`, `z_t`, and `x_t` are stored in the `cache`. At the end, we return the output `yhats` and `cache`.\n",
        "\n",
        "**Exercise [2/2]**: Implement `forward()` by stackin the `rnn_layer` and return the outputs `yhats` and `cache` that stores `hs`, `zs`, `yhats`, and `inputs`."
      ],
      "metadata": {
        "id": "P1WZbQQZBUNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(inputs, parameters, act=Tanh()):\n",
        "    # inputs: (batch_size, sequence_length, input_size)\n",
        "    # parameters: dictionary, containing (Wh, Wx, Wy, bh, by)\n",
        "    batch_size, sequence_length, input_size = inputs.shape\n",
        "    hidden_size = parameters['Wh'].shape[0]\n",
        "    output_size = parameters['Wy'].shape[0]\n",
        "\n",
        "    # Initialize outputs, hidden states, pre activation\n",
        "    yhats = np.zeros((batch_size, sequence_length, output_size))\n",
        "    hs = np.zeros((batch_size, sequence_length, hidden_size))\n",
        "    zs = np.zeros((batch_size, sequence_length, hidden_size))\n",
        "\n",
        "    # Initialize the previous hidden state\n",
        "    h_prev = np.zeros((batch_size, hidden_size))\n",
        "\n",
        "    # Loop through each time step\n",
        "    for t in range(sequence_length):\n",
        "        # Extract the input at time t and perform RNN layer calculations\n",
        "        ### Code Here ###\n",
        "\n",
        "        ### Code Here ###\n",
        "\n",
        "        # Store the results and Update the previous hidden state\n",
        "        ### Code Here ###\n",
        "\n",
        "        ### Code Here ###\n",
        "\n",
        "    # Cache the intermediate values\n",
        "    ### Code Here ###\n",
        "\n",
        "    ### Code Here ###\n",
        "\n",
        "    return yhats, cache"
      ],
      "metadata": {
        "id": "I-6Ks_fsF8wS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Test\n",
        "np.random.seed(1)\n",
        "batch_size = 7\n",
        "sequence_length = 2\n",
        "n_x = 3\n",
        "n_h = 9\n",
        "n_y = 5\n",
        "\n",
        "x = np.random.randn(batch_size, sequence_length, n_x)\n",
        "parameters = initialize_rnn_parameters(n_x, n_h, n_y)\n",
        "\n",
        "outputs, cache = forward(x, parameters)\n",
        "print(\"y_pred.shape:\", outputs.shape)\n",
        "print(\"y_pred:\", outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_3iELCSF8y7",
        "outputId": "cab57632-1bc7-4a4c-a93b-5746f3f06aa8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_pred.shape: (7, 2, 5)\n",
            "y_pred: [[[8.14030433e-01 1.85697573e-01 1.19428504e-04 1.50260441e-04\n",
            "   2.30558817e-06]\n",
            "  [9.15500527e-01 5.76525372e-04 8.38080431e-02 7.74767803e-05\n",
            "   3.74277242e-05]]\n",
            "\n",
            " [[2.25965257e-01 7.70775868e-01 1.28454261e-03 1.82887200e-03\n",
            "   1.45460715e-04]\n",
            "  [7.46652214e-01 9.38900248e-03 2.41129282e-01 2.58439814e-03\n",
            "   2.45103162e-04]]\n",
            "\n",
            " [[6.74700721e-04 2.42456728e-02 2.17135742e-02 1.73002039e-01\n",
            "   7.80364014e-01]\n",
            "  [8.76059591e-01 1.12555365e-01 9.12727590e-03 1.98127191e-03\n",
            "   2.76496978e-04]]\n",
            "\n",
            " [[7.75369988e-01 1.82259822e-02 1.86318767e-01 1.21015190e-02\n",
            "   7.98374302e-03]\n",
            "  [2.08883982e-05 9.38296085e-05 2.43152289e-02 2.77351744e-02\n",
            "   9.47834879e-01]]\n",
            "\n",
            " [[6.52061336e-01 3.45708061e-01 5.87547698e-04 1.58770786e-03\n",
            "   5.53476937e-05]\n",
            "  [4.03010060e-02 1.03779191e-02 7.59748679e-01 8.62274703e-02\n",
            "   1.03344925e-01]]\n",
            "\n",
            " [[9.56819502e-01 3.64582631e-02 3.55765118e-03 2.85283239e-03\n",
            "   3.11751088e-04]\n",
            "  [7.69509091e-01 1.84469051e-02 2.83176522e-02 9.55487248e-02\n",
            "   8.81776273e-02]]\n",
            "\n",
            " [[2.23020703e-07 6.61925876e-06 9.91300502e-03 4.96653305e-03\n",
            "   9.85113620e-01]\n",
            "  [1.52567204e-01 8.46430959e-01 8.59757875e-05 8.82382096e-04\n",
            "   3.34785553e-05]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-4 Compute Cost [1/1]\n",
        "\n",
        "Now let us compute the cost using cross entropy loss. Note that we have target sequence $y=(y_1, \\dots, y_T)$ and output sequence $\\bar{y}=(\\bar{y}_1, \\dots, \\bar{y}_T)$, where each $y_t$ and $\\bar{y}_t$ are a $n_y\\times 1$ vector. Their cross entropy loss at time $t$ is\n",
        "$$\n",
        "\\ell(y_t, \\bar{y}_t) = -y_t \\cdot \\log \\bar{y}_t\n",
        "$$\n",
        "Then the coss across all time is\n",
        "$$\n",
        "\\ell(y, \\bar{y}) = \\frac{1}{T}\\sum_{t} \\ell(y_t, \\bar{y}_t)\n",
        "$$\n",
        "Considering all training sample, we have\n",
        "$$\n",
        "L = \\frac{1}{N}\\sum_{i=1}^{N} \\ell(y^{(i)}, \\bar{y}^{(i)}) = \\frac{1}{NT}\\sum_{i=1}^{N}\\sum_{t=1}^{T} \\ell(y_t^{(i)}, \\bar{y}_t^{(i)})\n",
        "=-\\frac{1}{NT} \\sum_{i=1}^{N}\\sum_{t=1}^{T} y_t \\cdot \\log \\bar{y}_t\n",
        "$$\n",
        "\n",
        "**Exercise [1/1]**: Implemente the `compute_cost()`"
      ],
      "metadata": {
        "id": "JWVA5kXoK26S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost(targets, outputs):\n",
        "    # Compute the cross-entropy loss for the predicted outputs\n",
        "    # targets: (batch_size, sequence_length, output_size)\n",
        "    # outputs: (batch_size, sequence_length, output_size)\n",
        "    batch_size, sequence_length, output_size = targets.shape\n",
        "\n",
        "    # Compute the cross-entropy loss\n",
        "    ### Code Here ###\n",
        "\n",
        "    ### Code Here ###\n",
        "    return loss"
      ],
      "metadata": {
        "id": "YzHG_S5yK4Y0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_one_hot_targets(batch_size, sequence_length, n_y):\n",
        "    random_class_indices = np.random.randint(0, n_y, size=(batch_size, sequence_length))\n",
        "    one_hot_targets = np.zeros((batch_size, sequence_length, n_y))\n",
        "    one_hot_targets[np.arange(batch_size)[:, None], np.arange(sequence_length), random_class_indices] = 1\n",
        "    return one_hot_targets\n",
        "\n",
        "np.random.seed(123)\n",
        "targets = generate_one_hot_targets(batch_size, sequence_length, n_y)\n",
        "loss = compute_cost(targets, outputs)\n",
        "print(\"loss:\", loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XL6N2R06UDw0",
        "outputId": "28acd4e2-ee90-4542-9821-b186546b076f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 5.676070798625509\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-5 Backpropogation [3/3]"
      ],
      "metadata": {
        "id": "AXbaEOYnefFJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall that we have forward\n",
        "$$\n",
        "\\begin{align}\n",
        "z_t = &W_h h_{t-1} + W_x x_t + b_h\\\\\n",
        "h_t = &\\phi(z_t)\\\\\n",
        "u_t = & W_y h_t + b_y\\\\\n",
        "\\bar{y}_t = & \\text{softmax}(u_t)\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "Using chain rule, we have\n",
        "$$\n",
        "\\begin{align}\n",
        "&du_t= \\bar{y}_t-y_t\\\\\n",
        "&dW_y = d u_t h_t^{\\top}\\\\\n",
        "&db_y = d u_t\\\\\n",
        "&dh_t = W_y^{\\top} d u_t + W_h^{\\top} dz_{t+1} \\\\\n",
        "&dz_t = \\phi^{\\prime}(z_t) \\odot dh_t\\\\\n",
        "&dW_h = dz_t h_{t-1}^{\\top}\\\\\n",
        "&dW_x = dz_t x_t^{\\top}\\\\\n",
        "&db_h = dz_t\n",
        "\\end{align}\n",
        "$$\n",
        "We can see we need to pass $d z_t$ to the next time step for back propogate."
      ],
      "metadata": {
        "id": "BXt5FwZ4efHu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use vectorizaiton, suppose `x_t` is a batch of input with shape `(batch_size, input_size)`. Then the forward becomes\n",
        "$$\n",
        "\\begin{align}\n",
        "&Z_t = H_{t-1}W_h^{\\top} + X_t W_x^{\\top} + e b_h^{\\top} \\\\\n",
        "&H_t = \\phi(Z_t)\\\\\n",
        "&U_t = H_t W_y^{\\top} + e b_y^{\\top}\\\\\n",
        "&\\bar{Y}_t = \\text{softmax}(U_t)\n",
        "\\end{align}\n",
        "$$\n",
        "where\n",
        "$$\n",
        "\\begin{align}\n",
        "Z_t = \\begin{bmatrix}\n",
        "z_t^{(1)} & z_t^{(2)} & \\cdots & z_t^{(N)}\n",
        "\\end{bmatrix}\n",
        "^{\\top}\n",
        "\\in \\mathbb{R}^{N\\times n_h},\n",
        "\\quad\n",
        "e =\n",
        "\\begin{bmatrix}\n",
        "1 & 1 & \\cdots & 1\n",
        "\\end{bmatrix}^{\\top}\n",
        "\\in \\mathbb{R}^{N\\times 1}\n",
        "\\end{align}\n",
        "$$\n",
        "Then the gradients become\n",
        "$$\n",
        "\\begin{align}\n",
        "&dU_t =\\frac{1}{NT} (\\bar{Y}_t - Y_t)\\\\\n",
        "&dW_y = \\sum_t dU_t^{\\top} H_t\\\\\n",
        "&db_y = \\sum_t dU_t^{\\top} e\\\\\n",
        "&dH_t = dU_t W_y+ dZ_{t+1} W_h\\\\\n",
        "&dZ_t = \\phi^{\\prime}(Z_t) \\odot dH_t\\\\\n",
        "&dW_h = \\sum_t (dZ_t)^{\\top} H_{t-1}\\\\\n",
        "&dW_x = \\sum_t (dZ_t)^{\\top} X_t\\\\\n",
        "&db_h = \\sum_t (dZ_t)^{\\top} e\n",
        "\\end{align}\n",
        "$$\n",
        "**Note**: because the weights and baises are shared across all steps. The gradients are also accumulated across all time steps.\n",
        "\n",
        "**Exercise [3/3]**: Impelente `backward()` method and return `grads`."
      ],
      "metadata": {
        "id": "lCqDQHhRmOLO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def backward(y, parameters, caches, act_derivative=Tanh().derivative):\n",
        "    # y: (batch_size, sequence_length, output_size)\n",
        "    # parameters: dictionary, containing (Wh, Wx, Wy, bh, by)\n",
        "    # caches: dictionary, containing (yhats, hs, zs, inputs)\n",
        "\n",
        "    batch_size, sequence_length, output_size = y.shape\n",
        "    hidden_size = parameters['Wh'].shape[0]\n",
        "    input_size = parameters['Wx'].shape[1]\n",
        "\n",
        "    Wy = parameters['Wy']\n",
        "    Wh = parameters['Wh']\n",
        "    Wx = parameters['Wx']\n",
        "    bh = parameters['bh']\n",
        "    by = parameters['by']\n",
        "\n",
        "    # Initialize gradients\n",
        "    dWh = np.zeros_like(parameters['Wh'])\n",
        "    dWx = np.zeros_like(parameters['Wx'])\n",
        "    dWy = np.zeros_like(parameters['Wy'])\n",
        "    dbh = np.zeros_like(parameters['bh'])\n",
        "    dby = np.zeros_like(parameters['by'])\n",
        "    grads ={}\n",
        "\n",
        "    dz_t_next = np.zeros((batch_size, hidden_size))\n",
        "    for t in reversed(range(sequence_length)):\n",
        "        # Retrieve cached values for current time step\n",
        "        ### Code Here ###\n",
        "\n",
        "        ### Code Here ###\n",
        "\n",
        "        # Handle previous hidden state (use zeros if t=0)\n",
        "        ### Code Here ###\n",
        "\n",
        "        ### Code Here ###\n",
        "\n",
        "        # Compute output error\n",
        "        ### Code Here ###\n",
        "\n",
        "        ### Code Here ###\n",
        "\n",
        "        # Compute gradients for output weights and biases\n",
        "        ### Code Here ###\n",
        "\n",
        "        ### Code Here ###\n",
        "\n",
        "        # Backpropagate error to hidden state\n",
        "        ### Code Here ###\n",
        "\n",
        "        ### Code Here ###\n",
        "\n",
        "        # Compute gradient w.r.t. pre-activation z_t\n",
        "        ### Code Here ###\n",
        "\n",
        "        ### Code Here ###\n",
        "\n",
        "        # Compute gradients for hidden-to-hidden weights, input-to-hidden weights, and biases\n",
        "        ### Code Here ###\n",
        "\n",
        "        ### Code Here ###\n",
        "\n",
        "        # Update dz_t_next\n",
        "        ### Code Here ###\n",
        "\n",
        "        ### Code Here ###\n",
        "\n",
        "    grads['dWh'] = dWh\n",
        "    grads['dWx'] = dWx\n",
        "    grads['dWy'] = dWy\n",
        "    grads['dbh'] = dbh\n",
        "    grads['dby'] = dby\n",
        "\n",
        "    return grads"
      ],
      "metadata": {
        "id": "3r8fw45imNxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Test\n",
        "grads = backward(targets, parameters, cache)\n",
        "print(\"grads['dWx'].shape:\", grads['dWx'].shape)\n",
        "print(\"grads['dWx']:\", grads['dWx'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNdG4885p2p_",
        "outputId": "f5732668-4764-4aee-9b45-fc5b400b5912"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grads['dWx'].shape: (9, 3)\n",
            "grads['dWx']: [[-0.59072159  0.40433816  1.10746108]\n",
            " [ 0.958936    0.05239214  1.00942008]\n",
            " [-0.5482359   0.32950661 -1.2185552 ]\n",
            " [ 0.28912497  0.23787591 -3.48232618]\n",
            " [-1.60721343  0.91352167  0.52073181]\n",
            " [ 0.72894355 -0.49311288  0.55852002]\n",
            " [-0.04680902 -0.37165716  2.42702709]\n",
            " [ 1.07534461 -0.60867996  1.19172103]\n",
            " [-0.96090388  0.4181982   0.33683314]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.6 Build RNN [1/1]\n",
        "\n",
        "Now that we have all the components, it is time to build the `RecurrentNeuralNetwork` class. It has three essential methods\n",
        "- `initialize_parameters` random initialize each weights and zero initialize biases\n",
        "- `forward()`: given `inputs`, we use `self.parameters` to compute the `outputs` and return. Meanwhile, we store the intermediate values in `self.cache`.\n",
        "- `backward()`: Employ backpropogation through time to compute the gradients for weights and biases.\n",
        "\n",
        "Exercise [1/1]: Implement the `RecurrentNeuralNetwork` class."
      ],
      "metadata": {
        "id": "UGoLe6YPXCuY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RecurrentNeuralNetwork:\n",
        "    def __init__(self, input_size, output_size, hidden_size, act=Tanh()):\n",
        "        # Save input parameters to attributes and initialize parameters\n",
        "        ### Code Here ###\n",
        "\n",
        "        ### Code Here ###\n",
        "\n",
        "    def initialize_parameters(self):\n",
        "        self.parameters = {}\n",
        "        ### Code Here ###\n",
        "\n",
        "        ### Code Here ###\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        self.caches = {}\n",
        "        ### Code Here ###\n",
        "\n",
        "        ### Code Here ###\n",
        "        return outputs\n",
        "\n",
        "    def backward(self, y):\n",
        "        self.grads = {}\n",
        "        ### Code Here ###\n",
        "\n",
        "        ### Code Here ###\n",
        "        self.grads = grads\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        return self.forward(inputs)"
      ],
      "metadata": {
        "id": "C7G1_zA6xaVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Recurrent NeuralNetwork and its forward()\n",
        "np.random.seed(1)\n",
        "n_x = 3\n",
        "n_h = 9\n",
        "n_y = 5\n",
        "batch_size = 4\n",
        "sequence_length = 2\n",
        "x = np.random.randn(batch_size, sequence_length, n_x)\n",
        "y = generate_one_hot_targets(batch_size, sequence_length, n_y)\n",
        "\n",
        "network = RecurrentNeuralNetwork(n_x, n_y, n_h)\n",
        "\n",
        "outputs = network(x)\n",
        "print(f\"Shape of outputs: {outputs.shape}\")\n",
        "# print(f\"outputs = \\n{outputs}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Ms-0Mj1yI8P",
        "outputId": "eb42625e-5b62-4b44-d296-3c3eb181229b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of outputs: (4, 2, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the backward() method in NeuralNetwork\n",
        "network.backward(y)\n",
        "print(f\"Shape of grads: {network.grads['dWx'].shape}\")\n",
        "print(f\"grads['dWx'] = \\n{network.grads['dWx']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "239a4qU_yXe5",
        "outputId": "2478f31d-f563-48d5-d802-b63c783572f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of grads: (9, 3)\n",
            "grads['dWx'] = \n",
            "[[-0.33408226  0.20717265 -0.17950987]\n",
            " [ 0.42339331  0.24859148  0.09659816]\n",
            " [ 0.85870903 -0.41918044  0.00496727]\n",
            " [ 0.45031315 -0.47297622  0.71640565]\n",
            " [ 0.52565742 -0.23802471  0.01545178]\n",
            " [ 0.06318471 -0.34500968  0.19753802]\n",
            " [-0.01849903  0.19205179  0.17765888]\n",
            " [ 0.23638977 -0.05530361 -0.19717733]\n",
            " [ 0.67169325 -0.48560953  0.41511517]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 - Training RNN using Gradient Descent\n",
        "\n",
        "We now can train the RNN using gradient descent."
      ],
      "metadata": {
        "id": "bHCEmx9uz171"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent_step(parameters, grads, learning_rate):\n",
        "    for key in parameters.keys():\n",
        "        parameters[key] -= learning_rate * grads[f'd{key}']"
      ],
      "metadata": {
        "id": "fbHIr_epyriY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gradient_descent_step(network.parameters, network.grads, learning_rate=0.01)\n",
        "\n",
        "for key in network.parameters:\n",
        "    print(f\"{key} = \\n{network.parameters[f'{key}']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q79DC18T0D9M",
        "outputId": "a8c73396-ec0b-4662-cda4-7520ca3c3cc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wx = \n",
            "[[-0.01985759 -0.32381429  0.0294375 ]\n",
            " [-0.4283655  -0.11013178  0.21877389]\n",
            " [ 0.61532254  0.80300178  0.1951748 ]\n",
            " [ 0.04421935 -0.34532894 -0.31822794]\n",
            " [ 0.25573856  0.02178268  1.271471  ]\n",
            " [-0.21800838  0.14453404 -0.17569076]\n",
            " [-0.9137896   0.71471729 -0.10963308]\n",
            " [ 0.34313437  0.85245801  0.21690564]\n",
            " [ 0.39492141 -0.77987877 -0.36946202]]\n",
            "Wh = \n",
            "[[ 0.10986031  0.3496242   0.44979252  0.41524094 -0.23131324  0.42734633\n",
            "  -0.24182157 -0.26638211 -0.00153421]\n",
            " [ 0.17096413 -0.0548569   0.44763083 -0.20553441 -0.24781734  0.33887128\n",
            "  -0.18461997  0.14480114 -0.04988595]\n",
            " [ 0.00799085 -0.08548738  0.6127309  -0.06509604  0.52612364  0.25211712\n",
            "  -0.17554081 -0.25126458  0.02946636]\n",
            " [ 0.56049949 -0.06302826 -0.05351696 -0.45793266  0.01562913 -0.52317622\n",
            "  -0.43101348  0.02469749 -0.29477515]\n",
            " [-0.07760832 -0.31835933  0.00785813 -0.32343905 -0.25461273 -0.08842911\n",
            "  -0.05770037 -0.62784493  0.5360078 ]\n",
            " [ 0.38216656 -0.16221317  0.34092179 -0.32828395 -0.41293664  0.64027094\n",
            "  -0.14951357  0.51540084 -0.63782513]\n",
            " [-0.331346   -0.13554117  0.03370381 -0.28909834  0.12165935 -0.31296643\n",
            "  -0.4624453  -0.11332773 -0.05977637]\n",
            " [-0.08033296 -0.08249913 -0.05623839  0.19120769 -0.31898973 -0.21712552\n",
            "   0.23985408  0.41145335 -0.08891493]\n",
            " [ 0.52267962 -0.32835657  0.02762499  0.00970381 -0.38607831 -0.36657855\n",
            "  -0.0409012  -0.24448924  0.2266115 ]]\n",
            "Wy = \n",
            "[[-1.1509867  -2.70785963  0.33763363  1.89973091  1.02847637  2.83459239\n",
            "   3.11818766 -2.39918769  4.53899388]\n",
            " [ 0.4360091   1.33122474 -2.38190165  4.3178705  -1.3831266  -1.51272681\n",
            "   3.78791469 -2.23010359 -1.37526698]\n",
            " [-0.11224725 -1.1086377  -1.31274024  3.07604818 -1.09776538  0.25854946\n",
            "  -1.69262062 -0.58312581  1.4589016 ]\n",
            " [-0.12766189 -0.91789953 -5.44624739  0.02015283 -2.57381152  0.23709946\n",
            "   2.66270509 -2.35274898 -3.53495414]\n",
            " [-0.67905862  1.04181555 -5.01694331 -2.28537271 -2.69671482  0.55621664\n",
            "  -0.32703492  0.7893244   1.52645133]]\n",
            "bh = \n",
            "[[ 0.00441775]\n",
            " [ 0.00093143]\n",
            " [-0.00193035]\n",
            " [-0.00258762]\n",
            " [-0.00288833]\n",
            " [-0.00309124]\n",
            " [-0.00920422]\n",
            " [ 0.00148524]\n",
            " [ 0.00653478]]\n",
            "by = \n",
            "[[-0.00085045]\n",
            " [-0.00016961]\n",
            " [ 0.00106684]\n",
            " [-0.00045858]\n",
            " [ 0.0004118 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(network, inputs, targets, learning_rate=0.01, max_iter=100):\n",
        "    losses = []\n",
        "    for i in range(max_iter):\n",
        "        outputs = network(inputs)\n",
        "        losses.append(compute_cost(outputs, targets))\n",
        "        network.backward(y)\n",
        "        gradient_descent_step(network.parameters, network.grads, learning_rate)\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Iteration {i}: Loss = {losses[-1]}\")\n",
        "    return losses"
      ],
      "metadata": {
        "id": "pbFnaIyI0r7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(1)\n",
        "n_x = 3\n",
        "n_h = 100\n",
        "n_y = 5\n",
        "batch_size = 10\n",
        "sequence_length = 50\n",
        "x = np.random.randn(batch_size, sequence_length, n_x)\n",
        "y = generate_one_hot_targets(batch_size, sequence_length, n_y)\n",
        "\n",
        "network = RecurrentNeuralNetwork(n_x, n_y, n_h)\n",
        "losses = train(network, x, y, learning_rate=0.01, max_iter=1000)\n",
        "plt.plot(losses)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        },
        "id": "dhVoEcGXld4v",
        "outputId": "69e5cc35-a4c7-479c-ea7d-459de550265b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0: Loss = 18.15663669722336\n",
            "Iteration 100: Loss = 0.8782191175534547\n",
            "Iteration 200: Loss = 0.5335292699306723\n",
            "Iteration 300: Loss = 0.4342417973404307\n",
            "Iteration 400: Loss = 0.37848605036261607\n",
            "Iteration 500: Loss = 0.3405588773870283\n",
            "Iteration 600: Loss = 0.3119290213417056\n",
            "Iteration 700: Loss = 0.2888488852341271\n",
            "Iteration 800: Loss = 0.26941228378657167\n",
            "Iteration 900: Loss = 0.25255578204420387\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xuR7176gliiN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}