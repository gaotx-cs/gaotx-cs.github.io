{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Homework 2: Neural Network Training"
      ],
      "metadata": {
        "id": "Lr6Wlxltv6Z-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this assignment, you will implement a neural network to solve a real-world binary classification problem. The exercises will guide you through the following tasks:\n",
        "\n",
        "- Implement a Two-Layer Neural Network: Build a simple neural network with one hidden layer to classify data into two categories.\n",
        "- Random Initialization: Properly initialize the network’s weights and biases to ensure efficient training.\n",
        "- Compute the Cost using Square Loss: Implement the square loss function to evaluate the network’s performance.\n",
        "- Implement Forward and Backward Propagation: Develop the forward propagation to compute the output and the backward propagation to update the network’s parameters using gradient descent."
      ],
      "metadata": {
        "id": "5pultqe_J69h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0 - Packages\n",
        "Let's first import necessary libraries\n",
        "- [numpy](https://www.numpy.org/) is the fundamental package for scientific computing with Python.\n",
        "tools for data mining and data analysis.\n",
        "- [matplotlib](http://matplotlib.org) is a library for plotting graphs in Python."
      ],
      "metadata": {
        "id": "iGEg1rJSruUy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(1) # set a seed so that the results are consistent"
      ],
      "metadata": {
        "id": "lVhxpgqHuD-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1- Definining the neural network structure\n",
        "\n",
        "In this exercise, you will implement a two-layer neural network, also known as a multilayer perceptron (MLP), with one hidden layer. Given a training sample $(x,y)$, the forward propagation of the network is defined as follows:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "z^1 =& W^1 x + b^1\\\\\n",
        "a^1 =& \\phi(z^1)\\\\\n",
        "z^2 =& W^2 a^1 + b^2\\\\\n",
        "a^2 =& \\phi(z^2)\n",
        "\\end{align*}\n",
        "$$\n",
        "where\n",
        "- $W^i$ are the weights\n",
        "- $b^i$ are the bias\n",
        "- $z^i$ are the pre-activaiton,\n",
        "- $a^i$ are the activaiton\n",
        "\n",
        "The network's output is $a^2$, that is then compare to the true label $y$ using the square loss function:\n",
        "$$\n",
        "\\ell(a,y) = \\frac{1}{2}(a-y)^2\n",
        "$$"
      ],
      "metadata": {
        "id": "aeN8KBzsJQ00"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 1 [10/10]**:\n",
        "Define three values:\n",
        "- `n_x`: the size of the input data\n",
        "- `n_h`: the size of hidden layer, i.e., the number neurons in the hidden layer. The default value is $5$\n",
        "- `n_y`: the size of the output\n",
        "\n"
      ],
      "metadata": {
        "id": "KX4jyaK9LS07"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def neural_network_structure(X, Y, n_h=5):\n",
        "    n_x = X.shape[0]\n",
        "    ### Code star here ### (~ 1 lines of code)\n",
        "\n",
        "    ### End code here ###\n",
        "    return (n_x, n_h, n_y)"
      ],
      "metadata": {
        "id": "pm3uIAqRImDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.random.randn(2, 3)\n",
        "Y = np.random.randn(1, 3)\n",
        "n_x, n_h, n_y = neural_network_structure(X, Y, 10)\n",
        "\n",
        "print(\"The size of the input data: n_x = \" +str(n_x))\n",
        "print(\"The size of the hidden layer: n_h = \" +str(n_h))\n",
        "print(\"The size of the output: n_y = \" +str(n_y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3keagCFJMrI3",
        "outputId": "8dda3d40-b848-4a54-8bc7-c03e0526f275"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The size of the input data: n_x = 2\n",
            "The size of the hidden layer: n_h = 10\n",
            "The size of the output: n_y = 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 - Random Initialization\n",
        "\n",
        "**Excecise 2 [10/10]**: implement the function `initialize_parameters()`.\n",
        "To avoid symmetric patterns in neural networks, we’ll use random initialization for the weights.\n",
        "\n",
        "1. The function `initialize_parameters()` has input `n_x`, `n_h`, `n_y` as inputs.\n",
        "2. Use random normal distribution: `stdv * np.random.randn(a.b) + mu`, where `mu=0.0` and `stdv=1/np.sqrt(n_x)` for `W_1` and `stdv=1/np.sqrt(n_h)` for `W_2`\n",
        "3. Initialize biases as zeros with the correct shape: `np.zeros((a,b))`\n",
        "4. Return `parameters` as a dictionary containing all weights and biases."
      ],
      "metadata": {
        "id": "lJ-hQEQHNZvm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_parameters(n_x, n_h, n_y):\n",
        "    W1 = np.random.randn(n_h, n_x) / np.sqrt(n_x)\n",
        "    b1 = np.zeros((n_h, 1))\n",
        "    ### Code star here ### (~ 2 lines of code)\n",
        "\n",
        "    ### End code here ###\n",
        "\n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"b1\": b1,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2}\n",
        "    return parameters"
      ],
      "metadata": {
        "id": "NRHewuF9MzYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parameters = initialize_parameters(n_x, n_h, n_y)\n",
        "W1, b1, W2, b2 = parameters['W1'], parameters['b1'], parameters['W2'], parameters['b2']\n",
        "\n",
        "print(\"W1 = \" + str(W1))\n",
        "print(\"b1 = \" + str(b1))\n",
        "print(\"W2 = \" + str(W2))\n",
        "print(\"b2 = \" + str(b2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBD44YXmPESA",
        "outputId": "2cd4322c-f80f-42a6-dc85-ae43d37f4991"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W1 = [[-0.17633148  1.03386644]\n",
            " [-1.45673947 -0.22798339]\n",
            " [-0.27156744  0.80169606]\n",
            " [-0.77774057 -0.12192515]\n",
            " [-0.62073964  0.02984963]\n",
            " [ 0.41211259 -0.77825528]\n",
            " [ 0.8094419   0.63752091]\n",
            " [ 0.35531715  0.63700135]\n",
            " [-0.48346861 -0.08689651]\n",
            " [-0.66168891 -0.18942548]]\n",
            "b1 = [[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]\n",
            "W2 = [[ 0.16771312 -0.21872233 -0.12546448 -0.21730309 -0.26727749 -0.21226666\n",
            "  -0.0040049  -0.35332456  0.07412875  0.52487553]]\n",
            "b2 = [[0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3 - Sigmoid Function and Its Derivatives\n",
        "As discussed in the lectures, the step function is unsuitable for training MLPs because its derivative is zero almost everywhere. Instead, we’ll use the sigmoid function as the activation function."
      ],
      "metadata": {
        "id": "ss4CnKVeP7pW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 3 [10/10]**:\n",
        "1. Implement sigmoid function `sigmoid()` as $\\sigma(x)=\\frac{1}{1+e^{-x}}$\n",
        "2. Implement its derivative `sigmoid_derivative()` as $\\sigma^{\\prime}(x) = \\sigma(x) \\cdot (1-\\sigma(x))$"
      ],
      "metadata": {
        "id": "nAA8QsqdQe2t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "    ### Code star here ### (~ 1 lines of code)\n",
        "\n",
        "    ### End code here ###\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))"
      ],
      "metadata": {
        "id": "RCiC1PCKPg2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.linspace(-5,5, 10)\n",
        "s = sigmoid(x)\n",
        "s_d = sigmoid_derivative(x)\n",
        "\n",
        "print(s)\n",
        "print(s_d)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQL1_Pp2R1Cv",
        "outputId": "18395cbc-0b65-4df6-ed91-7546be8b9560"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.00669285 0.02005754 0.0585369  0.1588691  0.36457644 0.63542356\n",
            " 0.8411309  0.9414631  0.97994246 0.99330715]\n",
            "[0.00664806 0.01965523 0.05511033 0.13362971 0.23166046 0.23166046\n",
            " 0.13362971 0.05511033 0.01965523 0.00664806]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4 - Forward Propogation\n",
        "\n",
        "In the lecture, we covered the forward propagation for a 2-layer MLP using vectorization:\n",
        "$$\n",
        "\\begin{align}\n",
        "Z^1 &= W^1 * X + b^1\\\\\n",
        "A^1 &= \\phi(Z^1)\\\\\n",
        "Z^2 &= W^2 * A^1 + b^2\\\\\n",
        "A^2 &= \\phi(Z^2)\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "*Note*: NumPy’s broadcasting mechanism allows a bias vector `b` (shape `(n_h, 1)`) to be automatically added to each column of $W \\times A$ (shape `(n_h, m)`), where `m` is the number of training samples.\n",
        "\n",
        " **Exercise 4 [10/10]**: Implementing forward propagation `forward_propagation()`\n",
        "\n",
        " 1. The function `forward_propagation()` takes `X` and `parameters` as inputs\n",
        " 2. Retrieve the weights and bias from `parameters`\n",
        " 3. Compute `Z1`, `A1`, `Z2`, and `A2` using the equations above.\n",
        " 4. Store intermediate variables in `cache` for use in backpropagation."
      ],
      "metadata": {
        "id": "W6ty0zEmSQ9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_propagation(X, parameters):\n",
        "    # Retrieve each parameter from the dictionary \"parameters\"\n",
        "    W1 = parameters[\"W1\"]\n",
        "    b1 = parameters[\"b1\"]\n",
        "    ### Code star here ### (~ 2 lines of code)\n",
        "\n",
        "\n",
        "    ### End code here ###\n",
        "\n",
        "    # Implement Forward Propagation to calculate A2\n",
        "    Z1 = W1 @ X + b1\n",
        "    A1 = sigmoid(Z1)\n",
        "    ### Code star here ### (~ 2 lines of code)\n",
        "\n",
        "\n",
        "    ### End code here ###\n",
        "\n",
        "    # Store the intermedaite valeus in \"cache\" for backpropagation\n",
        "    cache = {\"Z1\": Z1,\n",
        "             \"A1\": A1,\n",
        "             \"Z2\": Z2,\n",
        "             \"A2\": A2}\n",
        "    return A2, cache\n"
      ],
      "metadata": {
        "id": "D8_VUXS3R4x6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A2, cache = forward_propagation(X, parameters)\n",
        "\n",
        "print(np.mean(cache['Z1']) ,np.mean(cache['A1']),np.mean(cache['Z2']),np.mean(cache['A2']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVayI3sdVaky",
        "outputId": "ae6417f3-c059-4166-ea8b-e2adc41b350b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-0.19151237249896635 0.4688525159515502 -0.3118444809339782 0.42266909957970195\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5 - Compute the Cost\n",
        "With the output estimate `A2` from forward propagation, we compute the cost using the square loss:\n",
        "$$\n",
        "L(\\theta)=\\frac{1}{2m} \\sum_{i=1}^{m} (a_i-y_i)^2\n",
        "$$\n",
        "\n",
        "**Exercise 5 [10/10]**: Implement `compute_cost()`\n"
      ],
      "metadata": {
        "id": "G3De3ZhSWHBj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def computer_cost(A2, Y):\n",
        "    m = Y.shape[1]\n",
        "    ### Code star here ### (~ 1 lines of code)\n",
        "\n",
        "    ### End code here ###\n",
        "    return cost"
      ],
      "metadata": {
        "id": "VCbyvFrbVxCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"cost = {computer_cost(A2, Y)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuKAsSC2Wrz7",
        "outputId": "6d8ea601-34a3-441e-dbdd-5104c63950d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cost = 0.5239053069310721\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6 - Backpropagation\n",
        "Using the `cache` computed during the forward propogation, we can compute the gradients through backpropogation\n",
        "$$\n",
        "\\begin{align}\n",
        "&d Z^2 = \\frac{1}{m}( A^{2} - Y) \\odot \\phi^{\\prime}( Z^{2}) \\\\\n",
        "&d W^2 = d Z^2 (A^1)^{\\top}\\\\\n",
        "&d b^2 = \\sum_{i=1}^{m} dZ^2_i\\\\\n",
        "&d Z^1 = ((W^2)^{\\top} dZ^2 ) \\odot \\phi^{\\prime}(Z^1)\\\\\n",
        "&d W^1 = d Z^1 X^{\\top}\\\\\n",
        "&d b^1 = \\sum_{i=1}^{m} dZ^1_i\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "**Exercise 6 [10/10]:** Implement `back_propogation()`\n",
        "1. The function `back_propogation()` takes data `X` and `Y`, weights and biases in `parameters`, and `cache` as inputs\n",
        "2. Retrive weights (`W1` and `W_2`) and biase (`b1` and `b2`) from `parameters`\n",
        "3. Retrive cached variables (`Z1`, `Z2`, `A1`, and `A2`) from `cache`\n",
        "4. Compute the gradients `dW1`, `dW2`, `db1`, `db2` using the provided formulas and you may also need to compute `dZ2` and `dZ1` as needed\n",
        "5. Return gradients in a variable `grads`\n",
        "\n",
        "*Note*: when implement `db1` or `db2`, you may consider use `np.sum()`. Let `M` be a matrix with shape `(a,b)`. Then `np.sum(M, axis=0)` sums each column, while `np.sum(M, axis=1)` sums each row. Use `keepdims=True` to maintain the dimensions summing. For example, `np.sum(M, axis=1, keepdims=True)` is used to sum across rows while preserving the shape needed for broadcasting."
      ],
      "metadata": {
        "id": "64bUYo3BW9Hm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def back_propogation(X, Y, parameters, cache):\n",
        "  # Retrieve each parameter from the dictionary \"parameters\"\n",
        "  W1 = parameters[\"W1\"]\n",
        "  b1 = parameters[\"b1\"]\n",
        "  W2 = parameters[\"W2\"]\n",
        "  b2 = parameters[\"b2\"]\n",
        "\n",
        "  # Retrieve each value from the dictionary \"cache\"\n",
        "  Z1 = cache[\"Z1\"]\n",
        "  A1 = cache[\"A1\"]\n",
        "  Z2 = cache[\"Z2\"]\n",
        "  A2 = cache[\"A2\"]\n",
        "\n",
        "  # Compute gradients: dW1, db1, dW2, db2\n",
        "  m = Y.shape[1]\n",
        "  dZ2 = (A2 - Y)/m * sigmoid_derivative(Z2)\n",
        "  dW2 = dZ2 @ A1.T\n",
        "  db2 = np.sum(dZ2, axis=1, keepdims=True)\n",
        "  ### Code star here ### (~ 3 lines of code)\n",
        "\n",
        "\n",
        "\n",
        "  ### Code end here ###\n",
        "\n",
        "  # Stores the gradients\n",
        "  grads = {\"dW1\": dW1,\n",
        "           \"db1\": db1,\n",
        "           \"dW2\": dW2,\n",
        "           \"db2\": db2}\n",
        "  return grads"
      ],
      "metadata": {
        "id": "-2FEJ7VkWzbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grads = back_propogation(X, Y, parameters, cache)\n",
        "print (\"dW1 = \"+ str(grads[\"dW1\"]))\n",
        "print (\"db1 = \"+ str(grads[\"db1\"]))\n",
        "print (\"dW2 = \"+ str(grads[\"dW2\"]))\n",
        "print (\"db2 = \"+ str(grads[\"db2\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WfKEbi9XYI9",
        "outputId": "93662664-9753-47a1-aae1-96f609cb7f9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dW1 = [[-0.00664941  0.00551946]\n",
            " [ 0.00666033 -0.00568816]\n",
            " [ 0.00529232 -0.00427319]\n",
            " [ 0.01028577 -0.00799248]\n",
            " [ 0.01305662 -0.0099341 ]\n",
            " [ 0.008174   -0.0066037 ]\n",
            " [ 0.00021912 -0.00017832]\n",
            " [ 0.02062341 -0.01630933]\n",
            " [-0.004029    0.00306173]\n",
            " [-0.02714012  0.02102039]]\n",
            "db1 = [[ 4.00223279e-04]\n",
            " [-2.71719312e-03]\n",
            " [-3.78546949e-04]\n",
            " [-1.13124221e-03]\n",
            " [-1.15711387e-03]\n",
            " [-9.91044386e-04]\n",
            " [-1.93340979e-06]\n",
            " [ 8.46922963e-04]\n",
            " [ 1.30854178e-04]\n",
            " [ 1.60536914e-03]]\n",
            "dW2 = [[ 0.04935606  0.05904166  0.04547199  0.0361181   0.03417343 -0.05364016\n",
            "  -0.01991294  0.00616847  0.02248469  0.02777064]]\n",
            "db2 = [[-0.0032622]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7 - Update Weights and Biases Using Gradient Descent\n",
        "Gradient descents are performed using:\n",
        "$$\n",
        "\\theta \\leftarrow \\theta - \\eta d\\theta\n",
        "$$\n",
        "where $\\eta>0$ is the learning rate.\n",
        "\n",
        "**Exercise 7 [10/10]**: Implemente `update_parameters()`\n",
        "1. The function takes `parameters`, `grads`, and `learning_rate` as inputs\n",
        "2. Retrive weights and biases from `parameters`\n",
        "3. Retrive gradients from `grads`\n",
        "4. Update the weights and biases using the gradient descent rule.\n",
        "5. Store the updated weights and biases back into `parameters` and return them"
      ],
      "metadata": {
        "id": "aa8E6b1OdvLO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_parameters(parameters, grads, learning_rate):\n",
        "\n",
        "  # Retrieve each parameter from the dictionary \"parameters\"\n",
        "  W1 = parameters[\"W1\"]\n",
        "  b1 = parameters[\"b1\"]\n",
        "  ### Code star here ### (~ 2 lines of code)\n",
        "\n",
        "\n",
        "  ### End code here ###\n",
        "\n",
        "  # Retrive gradients from the dictionary \"grads\"\n",
        "  dW1 = grads[\"dW1\"]\n",
        "  db1 = grads[\"db1\"]\n",
        "  ### Code star here ### (~ 2 lines of code)\n",
        "\n",
        "\n",
        "  ### End code here ###\n",
        "\n",
        "  # Update weights and biases using gradient descent update rule\n",
        "  W1 = W1 - learning_rate * dW1\n",
        "  b1 = b1 - learning_rate * db1\n",
        "  ### Code star here ### (~ 2 lines of code)\n",
        "\n",
        "\n",
        "  ### End code here ###\n",
        "\n",
        "  # Store the updated weights and biases back into \"parameters\"\n",
        "  parameters = {\"W1\": W1,\n",
        "                \"b1\": b1,\n",
        "                \"W2\": W2,\n",
        "                \"b2\": b2}\n",
        "  return parameters"
      ],
      "metadata": {
        "id": "B9GLhBWVXYLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parameters = update_parameters(parameters, grads, 0.01)\n",
        "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
        "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
        "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
        "print(\"b2 = \" + str(parameters[\"b2\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqjnXKjOXYNt",
        "outputId": "1041031b-689b-4f16-9302-3acc952173a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W1 = [[-0.17626499  1.03381124]\n",
            " [-1.45680607 -0.22792651]\n",
            " [-0.27162036  0.80173879]\n",
            " [-0.77784343 -0.12184523]\n",
            " [-0.62087021  0.02994897]\n",
            " [ 0.41203085 -0.77818925]\n",
            " [ 0.80943971  0.6375227 ]\n",
            " [ 0.35511092  0.63716444]\n",
            " [-0.48342832 -0.08692713]\n",
            " [-0.66141751 -0.18963568]]\n",
            "b1 = [[-4.00223279e-06]\n",
            " [ 2.71719312e-05]\n",
            " [ 3.78546949e-06]\n",
            " [ 1.13124221e-05]\n",
            " [ 1.15711387e-05]\n",
            " [ 9.91044386e-06]\n",
            " [ 1.93340979e-08]\n",
            " [-8.46922963e-06]\n",
            " [-1.30854178e-06]\n",
            " [-1.60536914e-05]]\n",
            "W2 = [[ 0.16721956 -0.21931275 -0.1259192  -0.21766427 -0.26761923 -0.21173026\n",
            "  -0.00380577 -0.35338624  0.07390391  0.52459783]]\n",
            "b2 = [[3.262196e-05]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8 - Training Loop\n",
        "\n",
        "**Exercise 8 [10/10]**: Integrate the preivous parts into a function `train_loop()`.\n",
        "1. The function `train_loop()` takes input data `(X,Y)` and network size `n_h`, and `learning_rate` with `max_iteration` as inputs\n",
        "2. Retrive `(n_x,n_h,n_y)` using `neural_network_structure()`\n",
        "3. Initialize the parameters using `initialize_parameters()`\n",
        "4. Create a `for` loop to train the network by calling `forward_propagation()` to compute the `cost` and `cach`, `back_propagation()` to compute the `grads`, then `update_parameters()` to update `parameters`."
      ],
      "metadata": {
        "id": "0gb8WlNSfSD-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(X, Y, n_h, learning_rate, max_iteration, print_cost=True):\n",
        "  # Retrive (n_x, n_h, n_y)\n",
        "  n_x, n_h, n_y = neural_network_structure(X, Y, n_h)\n",
        "\n",
        "  # Initialize the parameters\n",
        "  parameters = initialize_parameters(n_x, n_h, n_y)\n",
        "\n",
        "  for iter in range(max_iteration):\n",
        "    # Forward propagation\n",
        "    ### Code star here ### (~ 1 lines of code)\n",
        "\n",
        "\n",
        "    ### End code here ###\n",
        "\n",
        "    # Compute loss\n",
        "    cost = computer_cost(A2, Y)\n",
        "    if print_cost:\n",
        "      print(f\"Epoch {iter}: Loss = {cost}\")\n",
        "\n",
        "    # Backward propagation\n",
        "    ### Code star here ### (~ 1 lines of code)\n",
        "\n",
        "\n",
        "    ### End code here ###\n",
        "\n",
        "    # Update parameters\n",
        "    ### Code star here ### (~ 1 lines of code)\n",
        "\n",
        "    ### End code here ###\n",
        "\n",
        "  return parameters"
      ],
      "metadata": {
        "id": "n3rHawCvXYQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parameters = train_loop(X, Y, 10, 0.01, 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxhXILwThvKo",
        "outputId": "7330087c-6fe5-4b55-cb5e-9c948eeb10e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Loss = 0.5413416581298806\n",
            "Epoch 1: Loss = 0.5411639794561593\n",
            "Epoch 2: Loss = 0.5409863314471564\n",
            "Epoch 3: Loss = 0.540808715111546\n",
            "Epoch 4: Loss = 0.5406311314554088\n",
            "Epoch 5: Loss = 0.5404535814821864\n",
            "Epoch 6: Loss = 0.5402760661926396\n",
            "Epoch 7: Loss = 0.5400985865848044\n",
            "Epoch 8: Loss = 0.5399211436539499\n",
            "Epoch 9: Loss = 0.5397437383925353\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.1 - Training on Real Dataset\n",
        "\n",
        "Let us test your MLP model on the MNIST dataset, which contains images of digits `0` through `9`. For simplicity, we will select only the digits `0` and `1` for binary classification."
      ],
      "metadata": {
        "id": "1M437dYXiHqo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Flatten the 28x28 images into vectors of 784 elements and normalize to [0, 1]\n",
        "X_train = X_train.reshape(X_train.shape[0], -1).T / 255.0  # Transpose to (in_features, num_samples)\n",
        "X_test = X_test.reshape(X_test.shape[0], -1).T / 255.0     # Transpose to (in_features, num_samples)\n",
        "\n",
        "# Select only the samples of class '0' and '1' for binary classification\n",
        "train_filter = (y_train == 0) | (y_train == 1)\n",
        "test_filter = (y_test == 0) | (y_test == 1)\n",
        "\n",
        "X_train_binary = X_train[:, train_filter]\n",
        "y_train_binary = y_train[train_filter].reshape(1, -1)  # Reshape to (1, num_samples)\n",
        "\n",
        "X_test_binary = X_test[:, test_filter]\n",
        "y_test_binary = y_test[test_filter].reshape(1, -1)  # Reshape to (1, num_samples)\n",
        "\n",
        "# Verify the shapes\n",
        "print(f\"Training data shape: {X_train_binary.shape}\")  # Should be (784, num_samples)\n",
        "print(f\"Training labels shape: {y_train_binary.shape}\")  # Should be (1, num_samples)\n",
        "print(f\"Testing data shape: {X_test_binary.shape}\")  # Should be (784, num_samples)\n",
        "print(f\"Testing labels shape: {y_test_binary.shape}\")  # Should be (1, num_samples)\n",
        "\n",
        "# Print out some example labels to verify\n",
        "print(\"Training labels:\", np.unique(y_train_binary))\n",
        "print(\"Testing labels:\", np.unique(y_test_binary))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhQuZPfdiGKd",
        "outputId": "85d769b5-a909-41ef-fc2d-b6b478ea43eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape: (784, 12665)\n",
            "Training labels shape: (1, 12665)\n",
            "Testing data shape: (784, 2115)\n",
            "Testing labels shape: (1, 2115)\n",
            "Training labels: [0 1]\n",
            "Testing labels: [0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the following code, we can visualize a few examples from the dataset"
      ],
      "metadata": {
        "id": "EnvA8cIB515M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select a few random indices\n",
        "indices = np.random.choice(X_train_binary.shape[1], size=5, replace=False)\n",
        "\n",
        "# Plot the images\n",
        "fig, axes = plt.subplots(1, 5, figsize=(10, 2))\n",
        "\n",
        "for i, idx in enumerate(indices):\n",
        "    image = X_train_binary[:, idx].reshape(28, 28)\n",
        "    label = y_train_binary[0, idx]\n",
        "    axes[i].imshow(image, cmap='gray')\n",
        "    axes[i].set_title(f\"Label: {label}\")\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "Y5RgdRBx5qBz",
        "outputId": "9bac023f-4d20-4b69-cbb4-5061011a12b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x200 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parameters = train_loop(X_train_binary, y_train_binary, 10, 0.01, 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8qLerYChvP2",
        "outputId": "c217504d-1fbf-4665-947a-5877e525b09c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Loss = 0.15423868236507246\n",
            "Epoch 1: Loss = 0.1538372075502308\n",
            "Epoch 2: Loss = 0.1534339482553399\n",
            "Epoch 3: Loss = 0.15302893320861544\n",
            "Epoch 4: Loss = 0.15262219214320855\n",
            "Epoch 5: Loss = 0.15221375579440194\n",
            "Epoch 6: Loss = 0.15180365589554054\n",
            "Epoch 7: Loss = 0.15139192517266983\n",
            "Epoch 8: Loss = 0.15097859733785732\n",
            "Epoch 9: Loss = 0.1505637070811758\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9 - Predictions\n",
        "Use your trained model to make predictions by building `predict()`.\n",
        "\n",
        "**Exercise 9 [10/10]:**\n",
        "1. The function `predict()` takes `X` and `parameters` as inputs\n",
        "2. Call `forward_propagation()` to obtain the output `A2`\n",
        "3. Assign labels using threshold `0.5`; label class `1` if `A2 > 0.5`"
      ],
      "metadata": {
        "id": "hfb-_P45m4K2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(X, parameters):\n",
        "    A2, cache = forward_propagation(X, parameters)\n",
        "    ### Code star here ### (~ 3 lines of code)\n",
        "\n",
        "\n",
        "    ### End code here ###\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "QcAuTkBghvST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = predict(X_train_binary, parameters)\n",
        "print(f\"Training Accuracy: {np.mean(predictions == y_train_binary)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iijKBDURohhx",
        "outputId": "230c6b79-11aa-4000-f9cb-8218fcdfc2d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy: 0.5323332017370707\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = predict(X_test_binary, parameters)\n",
        "print(f\"Testing Accuracy: {np.mean(predictions == y_test_binary)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhhGfU4iohkZ",
        "outputId": "8cdd18e0-a447-4096-df14-e4a022188df6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing Accuracy: 0.5366430260047281\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10 - Tuning Network Hyperparameters\n",
        "\n",
        "In this two-layer MLP, we have the following hyperparameters: network size `n_h`, `learning_rate`, and `max_iteration`. The choice of these values will influence the network's performance.\n",
        "\n",
        "**Exercise 10 [10/10]**: Experiment with different values for network size `n_h` to observe how the network size influences performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "FhpyVaKLow0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(1)\n",
        "network_sizes = [5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
        "for n_h in network_sizes:\n",
        "    parameters = train_loop(X_train_binary, y_train_binary, n_h, 0.01, 10, False)\n",
        "    ### Code star here ### (~ 4 lines of code)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ### End code here ###\n",
        "    print(f\"Network Size: {n_h}, Training Accuracy: {train_accuracy}, Testing Accuracy: {test_accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUlGtGoVp84g",
        "outputId": "6227bac2-dc80-4d71-f90c-6d8455849369"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Network Size: 5, Training Accuracy: 0.46766679826292934, Testing Accuracy: 0.46335697399527187\n",
            "Network Size: 10, Training Accuracy: 0.46766679826292934, Testing Accuracy: 0.46335697399527187\n",
            "Network Size: 20, Training Accuracy: 0.46766679826292934, Testing Accuracy: 0.46335697399527187\n",
            "Network Size: 30, Training Accuracy: 0.5323332017370707, Testing Accuracy: 0.5366430260047281\n",
            "Network Size: 40, Training Accuracy: 0.5323332017370707, Testing Accuracy: 0.5366430260047281\n",
            "Network Size: 50, Training Accuracy: 0.46766679826292934, Testing Accuracy: 0.46335697399527187\n",
            "Network Size: 60, Training Accuracy: 0.5323332017370707, Testing Accuracy: 0.5366430260047281\n",
            "Network Size: 70, Training Accuracy: 0.5323332017370707, Testing Accuracy: 0.5366430260047281\n",
            "Network Size: 80, Training Accuracy: 0.46766679826292934, Testing Accuracy: 0.46335697399527187\n",
            "Network Size: 90, Training Accuracy: 0.5323332017370707, Testing Accuracy: 0.5366430260047281\n",
            "Network Size: 100, Training Accuracy: 0.5323332017370707, Testing Accuracy: 0.5366430260047281\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Congratulations!\n",
        "\n",
        "Well done on completing the assignment! You’ve implemented a two-layer neural network from scratch and trained it to solve a real binary classification problem. This is a significant milestone in understanding how neural networks work.\n",
        "\n",
        "Feel free to play around with the code, adjust the hyperparameters, and observe how they affect the network’s performance. By experimenting, you’ll gain a deeper insight into how neural networks learn and how tuning can improve results. Keep pushing your boundaries, and remember that each experiment brings you one step closer to mastering machine learning. Great job, and keep up the excellent work! 🎉🚀"
      ],
      "metadata": {
        "id": "9N08749Low3R"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}