{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Homework 3: Optimization in Neural Network\n",
        "\n",
        "In this assignment, you will design and implement a deep neural network (DNN) to solve a binary classification problem. This task involves building a flexible DNN architecture where both the width (number of neurons per layer) and depth (number of layers) can be customized. Additionally, you will implement and test various advanced optimization algorithms to understand how they impact the training process and model performance, such as\n",
        "- Momentum,\n",
        "- RMSProp,\n",
        "- Adam,\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OxG6NM4KBppr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0 - Packages\n",
        "Let's first import necessary libraries\n",
        "- [numpy](https://www.numpy.org/) is the fundamental package for scientific computing with Python.\n",
        "tools for data mining and data analysis.\n",
        "- [matplotlib](http://matplotlib.org) is a library for plotting graphs in Python."
      ],
      "metadata": {
        "id": "qiCkqus_CQjm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(1) # set a seed so that the results are consistent"
      ],
      "metadata": {
        "id": "OdjTRZ1dCRhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 - Build up Deep Neural Network"
      ],
      "metadata": {
        "id": "kSACvVmwaz9o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 - Initialization\n",
        "\n",
        "We will initialize the parameters of a DNN using a given `layer_dims` array, which specifies the size of each layer, including the input size `n_x` and output size `n_y`. As to deal with **symmetric patterns**, we use random initialization for weights and zero initialization for biases.\n",
        "\n",
        "Given a weigh matrix $W^{\\ell}\\in \\mathbb{R}^{n^{\\ell}\\times n^{\\ell-1}}$, each entry is randomly initialized using an i.i.d. Gaussian distribution:\n",
        "$$\n",
        "W^{\\ell}_{ij}\\sim \\mathcal{N}(0,1/n^{\\ell-1})\n",
        "$$\n",
        "\n",
        "**Exercise 1**:\n",
        "1. The method `initialize_dnn_parameters` takes `layer_dims` array as input\n",
        "2. Loop through the array using `stdv * np.random.randn(a.b) + mu` to random initialize weights `parameters['W' + str(l)]` and using `np.zeros((a,1))` to initialize bias `parameters['b' + str(1)]`\n",
        "3. Return `parameters` as a dictionary containing all weights and biases."
      ],
      "metadata": {
        "id": "OG30a4DVdfn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize weights and biases\n",
        "def initialize_dnn_parameters(layer_dims):\n",
        "    L = len(layer_dims) # number of layers\n",
        "    parameters = {}\n",
        "    for l in range(1, L):\n",
        "        ### Code here ### (~ 1 lines of code)\n",
        "        parameters['W' + str(l)] = stdv * np.random.randn(layer_dims[l], layer_dims[l-1]) + mu\n",
        "        ### Code here ### (~ 1 lines of code)\n",
        "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
        "    return parameters"
      ],
      "metadata": {
        "id": "HO9GZPV_degc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test initialization\n",
        "n_x = 5\n",
        "n_h = 4\n",
        "n_y = 3\n",
        "layer_dims = [n_x, n_h, n_y]\n",
        "parameters = initialize_dnn_parameters(layer_dims)\n",
        "for l in range(1, len(layer_dims)):\n",
        "    print(f\"W{l} = \\n{parameters['W' + str(l)]}\")\n",
        "    print(f\"b{l} = \\n{parameters['b' + str(l)]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxjOYobCdenE",
        "outputId": "b3b0aadf-342b-44dc-8017-afca3a3f2b37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W1 = \n",
            "[[ 0.72642933 -0.27358579 -0.23620559 -0.47984616  0.38702206]\n",
            " [-1.0292794   0.78030354 -0.34042208  0.14267862 -0.11152182]\n",
            " [ 0.65387455 -0.92132293 -0.14418936 -0.17175433  0.50703711]\n",
            " [-0.49188633 -0.07711224 -0.39259022  0.01887856  0.26064289]]\n",
            "b1 = \n",
            "[[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]\n",
            "W2 = \n",
            "[[-0.55030959  0.57236185  0.45079536  0.25124717]\n",
            " [ 0.45042797 -0.34186393 -0.06144511 -0.46788472]\n",
            " [-0.13394404  0.26517773 -0.34583038 -0.19837676]]\n",
            "b2 = \n",
            "[[0.]\n",
            " [0.]\n",
            " [0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 - Activation Functions and Its Derivatives\n",
        "In the preivous assignment, we implemented sigmoid activaiton function. In this assignment, we will focus on ReLU activaiton:\n",
        "$$\n",
        "f(x) = \\max\\{0,x\\}\n",
        "$$\n",
        "and its derivative is given by\n",
        "$$\n",
        "f^{\\prime}(x) =\n",
        "\\begin{cases}\n",
        "1, & x\\geq 0\\\\\n",
        "0, & x < 0\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "\n",
        "**Exercise 2**:\n",
        "1. We will implement activaiton function in a `class` form that includes both evaluation in `forward()` and `derivative()`\n",
        "1. Implement `forward()` method for ReLU function `class` so `forward()` is automatically evaluted when ReLU is called.\n",
        "2. Implement its `derivative` to compute the gradient during backpropagation."
      ],
      "metadata": {
        "id": "LvmRflTufv8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Sigmoid activation function class\n",
        "class Sigmoid:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def derivative(self, x):\n",
        "        return self.forward(x) * (1 - self.forward(x))\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return self.forward(x)\n",
        "\n",
        "# Define ReLU activation function class\n",
        "class ReLU:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward(self, x):\n",
        "        ### Code here ### (~ 1 lines of code)\n",
        "\n",
        "        ### Code here ### (~ 1 lines of code)\n",
        "\n",
        "    def derivative(self, x):\n",
        "        ### Code here ### (~ 1 lines of code)\n",
        "\n",
        "        ### Code here ### (~ 1 lines of code)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return self.forward(x)\n"
      ],
      "metadata": {
        "id": "6dFrGNdj9pQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the forward and backward (derivative) of ReLU activaiton\n",
        "x = np.random.randn(10)\n",
        "print(f\"x = \\n{x}\")\n",
        "print(f\"ReLU(x) = \\n{ReLU()(x)}\")\n",
        "print(f\"ReLU'(x) = \\n{ReLU().derivative(x)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzMKQ1q2UlNn",
        "outputId": "c4d4dfe6-a3e3-4872-c8e2-88d66a7b830a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x = \n",
            "[-0.6871727  -0.84520564 -0.67124613 -0.0126646  -1.11731035  0.2344157\n",
            "  1.65980218  0.74204416 -0.19183555 -0.88762896]\n",
            "ReLU(x) = \n",
            "[0.         0.         0.         0.         0.         0.2344157\n",
            " 1.65980218 0.74204416 0.         0.        ]\n",
            "ReLU'(x) = \n",
            "[0 0 0 0 0 1 1 1 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 - Forward Propogation\n",
        "With $A^{0}=X$ and $\\hat{Y}=A^{L}$, the forward propagation becomes\n",
        "        \\begin{align*}\n",
        "            &Z^{\\ell} = W^{\\ell} A^{\\ell-1} + b^{\\ell},&\\forall \\ell\\in [L]\\\\\n",
        "            &A^{\\ell}\n",
        "            =\\phi(Z^{\\ell}), & \\forall \\ell\\in [L]\n",
        "        \\end{align*}\n",
        "\n",
        "**Exercies 3**:\n",
        "1. Implement a `layer` of DNN that take previous output `A_prev`, weights `W`, bias `b`, and activaiton `act` as input\n",
        "2. The `layer` should perform a linear transform followed by a nonlinear activaiton\n",
        "3. Return both the pre-activaiton `Z` and the activation `A`"
      ],
      "metadata": {
        "id": "FIyZ0yFx3Vg5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def layer(A_prev, W, b, act=ReLU()):\n",
        "  ### Code here ### (~ 1 lines of code)\n",
        "\n",
        "  ### Code here ### (~ 1 lines of code)\n",
        "  A = act(Z)\n",
        "  return A, Z"
      ],
      "metadata": {
        "id": "QHhRh9zWdepk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the nonlinear layer\n",
        "num_samples = 5\n",
        "X = np.random.randn(n_x, num_samples)\n",
        "Y = np.random.randn(n_y, num_samples)\n",
        "\n",
        "A_prev = X\n",
        "W1 = parameters['W1']\n",
        "b1 = parameters['b1']\n",
        "A1, Z1 = layer(A_prev, W1, b1)\n",
        "print(f\"Z1 = \\n{Z1}\")\n",
        "print(f\"A1 = \\n{A1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sELTGJfNdesQ",
        "outputId": "55ef0ebc-3867-4082-844c-aae191793ef2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Z1 = \n",
            "[[-1.09575907  1.0266851  -0.31827206 -0.3506713  -0.12633385]\n",
            " [ 2.87245507 -1.45533316  0.57223289  0.59096241 -0.70441952]\n",
            " [-2.15866841  0.8461243  -0.40963115 -0.68640041  0.68699072]\n",
            " [ 0.80537242 -0.77691677  0.15346325  0.02594094 -0.07751913]]\n",
            "A1 = \n",
            "[[0.         1.0266851  0.         0.         0.        ]\n",
            " [2.87245507 0.         0.57223289 0.59096241 0.        ]\n",
            " [0.         0.8461243  0.         0.         0.68699072]\n",
            " [0.80537242 0.         0.15346325 0.02594094 0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall the forward propagation:\n",
        "$$\n",
        "        \\begin{align*}\n",
        "            &Z^{\\ell} = W^{\\ell} A^{\\ell-1} + b^{\\ell},&\\forall \\ell\\in [L]\\\\\n",
        "            &A^{\\ell}\n",
        "            =\\phi(Z^{\\ell}), & \\forall \\ell\\in [L]\n",
        "        \\end{align*}\n",
        "$$\n",
        "Using the `layer` function, DNNs can be built by staking `layer`:\n",
        "\n",
        "`layer_1 -> layer_2 -> ... -> layer_L`\n",
        "\n",
        "**Exercise 4**:\n",
        "In this exercise, you will stack multiple `layer` operations to build a DNN by implementing the `forward_propagation()` function.\n",
        "\n",
        " 1. The function `forward_propagation()` takes `X`, `parameters`, and `act` as inputs\n",
        " 2. Calculate the number of layers `L=len(parameters)//2`, since each layer has two parameters: weights and biases\n",
        " 3. For each layer, retrieve the weights and biases from parameters and apply the `layer()` function to propagate forward.\n",
        " 4. Store intermediate variables `Zl` and `Al` in `cache` for later use in backpropagation.\n",
        " 5. At the end, return the final output `AL` and the `cache`\n"
      ],
      "metadata": {
        "id": "aYrFV3_ygBlQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_propagation(X, parameters, act=ReLU()):\n",
        "  L = len(parameters) // 2 # num of layers\n",
        "  A = X # Initialize A0 with X\n",
        "  caches = {}\n",
        "  caches['A0'] = A # Cache the initial activaiton\n",
        "\n",
        "  for l in range(1, L+1): # including the output layer\n",
        "    A_prev = A\n",
        "    W = parameters['W' + str(l)]\n",
        "    b = parameters['b' + str(l)]\n",
        "    ### Code here ### (~ 1 lines of code)\n",
        "\n",
        "    ### Code here ### (~ 1 lines of code)\n",
        "    caches['Z' + str(l)] = Z\n",
        "    caches['A' + str(l)] = A\n",
        "\n",
        "  return A, caches"
      ],
      "metadata": {
        "id": "ce-oQYNliBA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test forward propagation\n",
        "A, caches = forward_propagation(X, parameters)\n",
        "\n",
        "L = len(parameters) // 2\n",
        "for l in range(1, L+1):\n",
        "    print(f\"Z{l} = \\n{caches['Z' + str(l)]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RL3MR4ZNiBDv",
        "outputId": "3dce9247-11f5-42c2-9629-6c42f1d710d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Z1 = \n",
            "[[-1.09575907  1.0266851  -0.31827206 -0.3506713  -0.12633385]\n",
            " [ 2.87245507 -1.45533316  0.57223289  0.59096241 -0.70441952]\n",
            " [-2.15866841  0.8461243  -0.40963115 -0.68640041  0.68699072]\n",
            " [ 0.80537242 -0.77691677  0.15346325  0.02594094 -0.07751913]]\n",
            "Z2 = \n",
            "[[ 1.84643125 -0.18356575  0.36608149  0.34476193  0.30969223]\n",
            " [-1.35881023  0.41045749 -0.2674289  -0.2141661  -0.04221222]\n",
            " [ 0.60194395 -0.43013384  0.12129988  0.15156399 -0.23758226]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 Compute the Cost\n",
        "\n",
        "With the output estimate `AL` from forward propagation, we compute the cost using the square loss:\n",
        "$$\n",
        "L(\\theta)=\\frac{1}{2m} \\sum_{i=1}^{m} (a^{L}_i-y_i)^2 = \\frac{1}{2m}\\|A^{L}-Y\\|^2\n",
        "$$\n",
        "The `compute_cost()` method is implemented in the previous programming assignment and it is copyed here."
      ],
      "metadata": {
        "id": "1KqgaijN5AkV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost(A, Y):\n",
        "    m = Y.shape[1]\n",
        "    cost = np.sum((A - Y) ** 2) / (2 * m)\n",
        "    return cost"
      ],
      "metadata": {
        "id": "yrhUn6A0iBF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the cost\n",
        "print(f\"cost = {compute_cost(A, Y)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44oqic2ciBID",
        "outputId": "d92ffaf0-1efd-417c-e2c9-dc379bdbe8df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cost = 1.7072853869205002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.5 - Backpropagation\n",
        "Using the `cache` computed during the `forward_propogation()`, we can compute the gradients using backpropogation.\n",
        "\n",
        "The gradient of the cost with respect to $Z^{L}$ is given by:\n",
        "$$d Z^{L} = \\frac{1}{m}( A^{L} -  Y) \\odot \\phi^{\\prime}(Z^{L}),$$\n",
        "Backpropagation for the gradients is then given by\n",
        "$$\n",
        "\\begin{align*}\n",
        "    &d W^{\\ell} = d Z^{\\ell}  * A^{(\\ell-1) \\top},&&\\forall \\ell\\in [L]\\\\\n",
        "    &d b^{\\ell} = d Z^{\\ell} * e,&&\\forall \\ell\\in [L]\\\\\n",
        "    &d Z^{\\ell-1} =\\phi^{\\prime}( Z^{\\ell-1}) \\odot \\left[ W^{(\\ell)\\top} d Z^{\\ell}\\right], &&\\forall \\ell\\in [2,3, ..., L]\\\\\n",
        "\\end{align*}\n",
        "$$\n",
        "Note that because we don't have $dZ^{0}$, so the backpropogate does not pass $dZ^{\\ell-1}$ for the first layer, when $\\ell=1$.\n",
        "\n",
        "**Exercise 4:** Implement `back_propogation()`\n",
        "1. The function `back_propogation()` takes data `Y`, `parameters`, and `caches` as inputs\n",
        "2. For each hidden layer, retrieve weights and biases from `parameters`, and intermediate variables `Zl` and `Al` from `caches`\n",
        "3. Compute the gradients `dWl`, `dbl`, using the formulas provided.\n",
        "4. Return the gradients in a variable `grads`\n"
      ],
      "metadata": {
        "id": "XySxmq-Q5Joc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def back_propagation(Y, parameters, caches, act_derivative=ReLU().derivative):\n",
        "    L = len(parameters) // 2 # num of layers in DNN\n",
        "    m = Y.shape[1]           # num of training samples\n",
        "    grads = {}\n",
        "\n",
        "    ZL = caches['Z' + str(L)]\n",
        "    AL = caches['A' + str(L)]\n",
        "    dZ = (AL - Y) / m * act_derivative(ZL)  # Derivative of loss w.r.t. A\n",
        "\n",
        "    for l in reversed(range(1, L+1)):\n",
        "        # Compute the gradients using dZ and A_prev\n",
        "        A_prev = caches['A' + str(l-1)]\n",
        "        ### Code here ### (~ 2 lines of code)\n",
        "\n",
        "\n",
        "        ### Code here ### (~ 2 lines of code)\n",
        "\n",
        "        # Store the computed gradients\n",
        "        grads['dW' + str(l)] = dW\n",
        "        grads['db' + str(l)] = db\n",
        "\n",
        "        # Backpropogate dZ\n",
        "        W = parameters['W' + str(l)]\n",
        "        if l > 1:\n",
        "            Z_prev = caches['Z' + str(l - 1)]\n",
        "            dZ = W.T @ dZ * act_derivative(Z_prev)\n",
        "\n",
        "    return grads"
      ],
      "metadata": {
        "id": "iN0A9R8XiBKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the backpropagation()\n",
        "grads = back_propagation(Y, parameters, caches)\n",
        "L = len(parameters) // 2\n",
        "for l in range(1, L+1):\n",
        "    print(f\"dW{l} = \\n{grads['dW' + str(l)]}\")\n",
        "    print(f\"db{l} = \\n{grads['db' + str(l)]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyEm2jjNiBMm",
        "outputId": "3f21bffb-0867-4e08-9cdf-5f3d8829a3a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dW1 = \n",
            "[[-0.07100506 -0.00504114  0.01465629 -0.0119815   0.01250615]\n",
            " [-0.1535268   0.2243261   0.06768908  0.00272228  0.07802137]\n",
            " [ 0.02370006 -0.02516877  0.05958518  0.09359966  0.0813598 ]\n",
            " [-0.06098153  0.1461884  -0.00877939  0.06790589  0.06303178]]\n",
            "db1 = \n",
            "[[-0.04195389]\n",
            " [ 0.41779308]\n",
            " [ 0.07912688]\n",
            " [ 0.21442471]]\n",
            "dW2 = \n",
            "[[ 0.          0.60078972  0.11186381  0.11598846]\n",
            " [-0.0956278   0.         -0.07880995  0.        ]\n",
            " [ 0.         -0.15898858  0.         -0.05413308]]\n",
            "db2 = \n",
            "[[ 0.93844522]\n",
            " [-0.09314229]\n",
            " [-0.09856997]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.6 - Define Deep Neural Networks\n",
        "As we have implemented `initialize_dnn_parameters()`, `forward_propagation()`, and `back_propogation()`, we will combine everthing and define the `NeuralNetork()` class.\n",
        "\n",
        "**Exercise 5**:\n",
        "1. To define the `NeuralNetwork` class, we will take input size `n_x`, output size `n_y`, and width `n_h`, the depth `depth`, and activaiton function `act` as inputs.\n",
        "2. Store these hyperparameters `n_x`, `n_y`, `n_h`, `depth`, `act` as attributes of the class\n",
        "3. Next, we `initialize()` DNN by using `initialize_dnn_parameters()` to randomly initialze the network's `self.parameters`\n",
        "4. When the DNN is called, it will automcatically run `forward()` using the `forward_propogation()` method, and store the intermediate pre-activatios and activaitons along in `self.caches`.\n",
        "5. The DNN also implements a `backward()` method using `back_propogation()` to compute the `grads` and store them into `self.grads` for use in training\n",
        "\n",
        "**Note**: for simplicity, we assume each hidden layer has the same width `n_h`, but in pratice it can vary based on the learning task"
      ],
      "metadata": {
        "id": "p99Q4eKG-luN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Neural Network class\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, n_x, n_y, n_h, depth, act=ReLU()):\n",
        "        self.n_x = n_x\n",
        "        self.n_y = n_y\n",
        "        self.n_h = n_h\n",
        "        self.depth = depth\n",
        "        self.act = act\n",
        "        self.initialize()\n",
        "\n",
        "    def initialize(self):\n",
        "        layer_dims = [self.n_x] + [self.n_h] * (self.depth-1) + [self.n_y]\n",
        "        ### Code here ### (~ 1 lines of code)\n",
        "\n",
        "        ### Code here ### (~ 1 lines of code)\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.caches = {}\n",
        "        output, caches = forward_propagation(X, self.parameters, self.act)\n",
        "        self.caches = caches\n",
        "        return output\n",
        "\n",
        "    def backward(self, Y):\n",
        "        self.grads = {}\n",
        "        ### Code here ### (~ 1 lines of code)\n",
        "\n",
        "        ### Code here ### (~ 1 lines of code)\n",
        "        self.grads = grads\n",
        "\n",
        "    def __call__(self, X):\n",
        "        return self.forward(X)"
      ],
      "metadata": {
        "id": "HCJ6c7rv-rol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test NeuralNetwork and its forward()\n",
        "network = NeuralNetwork(n_x, n_y, n_h, depth=3, act=Sigmoid())\n",
        "for l in range(1, network.depth+1):\n",
        "    print(f\"W{l} = \\n{network.parameters['W' + str(l)]}\")\n",
        "    print(f\"b{l} = \\n{network.parameters['b' + str(l)]}\")\n",
        "\n",
        "A = network(X)\n",
        "print(f\"A = \\n{A}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7oFiL2jA4cA",
        "outputId": "1fa648fa-fba3-4c8d-aefa-1608eff0f90e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W1 = \n",
            "[[ 0.08343279  0.18338067  0.08868233  0.05322228 -0.29992929]\n",
            " [ 0.16885166  0.05448013  0.50512056  0.53617238  0.08280447]\n",
            " [-0.16783253 -0.28564892  0.18939243  0.03458753 -0.15377604]\n",
            " [ 0.01949711 -0.27727281  0.31216942 -0.19996197  0.54761649]]\n",
            "b1 = \n",
            "[[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]\n",
            "W2 = \n",
            "[[ 0.20174582  0.29678926 -0.54745592  0.08469122]\n",
            " [ 0.37027823 -0.4768503  -0.13310925  0.01630727]\n",
            " [-0.68655866  0.1575797   0.42308032 -0.42975797]\n",
            " [ 0.17527299 -0.65614171 -0.01934775 -0.80788618]]\n",
            "b2 = \n",
            "[[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]\n",
            "W3 = \n",
            "[[ 0.56070885  0.20445027 -0.01230848 -0.38758081]\n",
            " [ 0.63687797  0.98355087 -0.92899093  0.61808202]\n",
            " [ 0.81382538  0.16900585 -0.59963402  0.43167266]]\n",
            "b3 = \n",
            "[[0.]\n",
            " [0.]\n",
            " [0.]]\n",
            "A = \n",
            "[[0.5594009  0.56163771 0.56270777 0.55963543 0.56721295]\n",
            " [0.65571946 0.64975006 0.64269171 0.64587559 0.62735589]\n",
            " [0.60243733 0.5987133  0.59506056 0.59312079 0.58729086]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the backward() method in NeuralNetwork\n",
        "network.backward(Y)\n",
        "\n",
        "for l in range(1, network.depth+1):\n",
        "    print(f\"dW{l} = \\n{network.grads['dW' + str(l)]}\")\n",
        "    print(f\"db{l} = \\n{network.grads['db' + str(l)]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8jVn1mVFDUW",
        "outputId": "b7d5c57b-9cad-486a-da28-c39a4bb9055f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dW1 = \n",
            "[[-0.00819391  0.00434696  0.01032395 -0.00024787  0.00653069]\n",
            " [ 0.00220476 -0.00447719 -0.00122303  0.00096706 -0.00096016]\n",
            " [ 0.00773704 -0.00218865 -0.01071998 -0.00014516 -0.00656312]\n",
            " [-0.0032138  -0.00055366  0.00339891  0.00027155  0.00214983]]\n",
            "db1 = \n",
            "[[ 0.01889197]\n",
            " [-0.00765741]\n",
            " [-0.0164768 ]\n",
            " [ 0.00279429]]\n",
            "dW2 = \n",
            "[[ 0.02987019  0.03644857  0.03402445  0.03874588]\n",
            " [ 0.02737154  0.03083942  0.02911464  0.03225519]\n",
            " [-0.02720202 -0.03051821 -0.02837608 -0.03141728]\n",
            " [ 0.01224843  0.0119922   0.01098373  0.01152867]]\n",
            "db2 = \n",
            "[[ 0.0630978 ]\n",
            " [ 0.05643017]\n",
            " [-0.05552861]\n",
            " [ 0.02358812]]\n",
            "dW3 = \n",
            "[[0.06034881 0.053968   0.05274028 0.03386175]\n",
            " [0.09777237 0.09016585 0.08312728 0.0634621 ]\n",
            " [0.03890758 0.03489111 0.03297021 0.02320944]]\n",
            "db3 = \n",
            "[[0.1204913 ]\n",
            " [0.19279404]\n",
            " [0.07642293]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 - Optimization Algorithms\n",
        "\n",
        "In this section, we will implement several optimization algorithms that improve the efficiency and effectiveness of training deep neural networks. Using the `class`-based approach, we will define various optimizers, such as Gradient Descent, Momentum, RMSProp, and Adam, which will be used to update the network’s weights. Each optimizer class will encapsulate the update rules specific to the algorithm, allowing for flexible and modular integration with the neural network during training."
      ],
      "metadata": {
        "id": "mYC2LEnhajGf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 - Gradient Descent\n",
        "\n",
        "Gradient descents are performed using the following rule:\n",
        "$$\n",
        "\\theta \\leftarrow \\theta - \\eta d\\theta\n",
        "$$\n",
        "where $\\eta>0$ is the learning rate.\n",
        "\n",
        "**Exercise 6**: Implemente `gradient_descent_step()`\n",
        "1. The function takes `parameters`, `grads`,  and `learning_rate` as inputs\n",
        "2. For each layer, it retrive weights and biases from `parameters`, gradients from `grads`\n",
        "3. Update the weights and biases using the gradient descent rule.\n",
        "4. Store the updated weights and biases back into `parameters`"
      ],
      "metadata": {
        "id": "lgGIWXbTFg1T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent_step(parameters, grads, learning_rate):\n",
        "    L = len(parameters) // 2\n",
        "    for l in range(1, L+1):\n",
        "      W, b = parameters['W' + str(l)], parameters['b' + str(l)]\n",
        "      dW, db = grads['dW' + str(l)], grads['db' + str(l)]\n",
        "\n",
        "      ### Code here ### (~ 1 lines of code)\n",
        "\n",
        "      ### Code here ### (~ 1 lines of code)\n",
        "      b -= learning_rate * db\n",
        "\n",
        "      parameters['W' + str(l)] = W\n",
        "      parameters['b' + str(l)] = b"
      ],
      "metadata": {
        "id": "a6dhu4A2GvRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gradient_descent_step(network.parameters, network.grads, learning_rate=0.01)\n",
        "\n",
        "for l in range(1, network.depth+1):\n",
        "    print(f\"W{l} = \\n{network.parameters['W' + str(l)]}\")\n",
        "    print(f\"b{l} = \\n{network.parameters['b' + str(l)]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ucmva98IHZSW",
        "outputId": "760c93a9-542e-46cf-a8c3-a4b174569586"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W1 = \n",
            "[[ 0.08351473  0.1833372   0.08857909  0.05322476 -0.2999946 ]\n",
            " [ 0.16882961  0.0545249   0.50513279  0.53616271  0.08281407]\n",
            " [-0.1679099  -0.28562704  0.18949963  0.03458898 -0.15371041]\n",
            " [ 0.01952925 -0.27726727  0.31213543 -0.19996469  0.547595  ]]\n",
            "b1 = \n",
            "[[-1.88919658e-04]\n",
            " [ 7.65740929e-05]\n",
            " [ 1.64767984e-04]\n",
            " [-2.79428845e-05]]\n",
            "W2 = \n",
            "[[ 0.20144712  0.29642478 -0.54779617  0.08430376]\n",
            " [ 0.37000451 -0.4771587  -0.1334004   0.01598472]\n",
            " [-0.68628664  0.15788488  0.42336408 -0.4294438 ]\n",
            " [ 0.17515051 -0.65626163 -0.01945759 -0.80800146]]\n",
            "b2 = \n",
            "[[-0.00063098]\n",
            " [-0.0005643 ]\n",
            " [ 0.00055529]\n",
            " [-0.00023588]]\n",
            "W3 = \n",
            "[[ 0.56010537  0.20391059 -0.01283588 -0.38791943]\n",
            " [ 0.63590024  0.98264922 -0.92982221  0.61744739]\n",
            " [ 0.8134363   0.16865694 -0.59996372  0.43144056]]\n",
            "b3 = \n",
            "[[-0.00120491]\n",
            " [-0.00192794]\n",
            " [-0.00076423]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we can testify the `gradient_descent_step` by creating a training `for` loop that applies `gradient_descent_step()` iteratively to update the `network.parameters` over multiple iterations."
      ],
      "metadata": {
        "id": "z2ar-WGkIn41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.1\n",
        "max_iter = 10\n",
        "losses = []\n",
        "for i in range(max_iter):\n",
        "    A = network(X)\n",
        "    network.backward(Y)\n",
        "    losses.append(compute_cost(A, Y))\n",
        "    gradient_descent_step(network.parameters, network.grads, learning_rate)\n",
        "\n",
        "losses = np.array(losses)\n",
        "print(f\"Losses: \\n{losses}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITeio9JwInRW",
        "outputId": "4104fcbd-272c-41b3-9a18-ed602088ee73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Losses: \n",
            "[2.24437118 2.23186699 2.21949885 2.20728322 2.19523579 2.18337136\n",
            " 2.17170374 2.16024557 2.14900828 2.13800201]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 - Gradient Descent with Momentum\n"
      ],
      "metadata": {
        "id": "BQnO16bJKFBH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient descent with momentum follows the updated rule:\n",
        "$$\n",
        "\\begin{align}\n",
        "v^{+} =& \\beta v + (1-\\beta) \\nabla L(v)\\\\\n",
        "w^{+} =& w - \\eta v^{+}\n",
        "\\end{align}\n",
        "$$\n",
        "where $\\beta$ is the momentum factor and $\\eta$ is the learning rate.\n",
        "\n",
        "It’s important to note that we cannot simply implement `gradient_descent_with_momentum_step()` by passing the `network.parameters` and `network.grads`. That is because the momentum-based method rely on the **historical** information, which must be stored across iterations. This requirement also applies to methods like **RMSProp** and **Adam**. Therefore, we create an `Optimizer()` class, which includes internal variables to store the necessary historical data and can be extended to other optimizers.\n",
        "\n",
        "**Steps**:\n",
        "1. The `Optimizer` class takes an neural network `network` and `learning_rate` as inputs for initialization\n",
        "2. It should store the network `network` and `learning_rate` as internal variables\n",
        "3. It also have a method called `step()` that applies different optimizer step"
      ],
      "metadata": {
        "id": "ILR7HhIqKFEe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Optimizer:\n",
        "    def __init__(self, network, learning_rate):\n",
        "        self.network = network  # The entire neural network is passed in\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def step(self):\n",
        "        raise NotImplementedError(\"Step method must be implemented in a subclass\")"
      ],
      "metadata": {
        "id": "Q2_dWjF7KELa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By using this basic class `Optimizer`, we can extend it to other optimizers by overriding the `step()` method. Let us take implement `GradientDescent` as an example.\n",
        "\n",
        "**Exercies 7**:\n",
        "1. Inherit `GradientDescent` from `Optimizer` class by using `class GradientDescent(Optimizer)`\n",
        "2. Since `GradientDescent` requires no additional inputs, we can omite the `__init__()` method\n",
        "3. Implement the `step()` using the `gradient_descent_step()` function."
      ],
      "metadata": {
        "id": "dLouBYbIOWGq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GradientDescent(Optimizer):\n",
        "    # No need to redefine __init__() because it uses the same as Optimizer\n",
        "\n",
        "    def step(self):\n",
        "        ### Code here ### (~ 1 lines of code)\n",
        "\n",
        "        ### Code here ### (~ 1 lines of code)"
      ],
      "metadata": {
        "id": "Olr_bDa8OVkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the `GradientDescent` optimizer by using the previous training loop\n",
        "optimizer = GradientDescent(network, learning_rate = 0.1)\n",
        "\n",
        "max_iter = 10\n",
        "losses = []\n",
        "for i in range(max_iter):\n",
        "    A = network(X)\n",
        "    network.backward(Y)\n",
        "    losses.append(compute_cost(A, Y))\n",
        "    optimizer.step()\n",
        "\n",
        "losses = np.array(losses)\n",
        "print(f\"Losses: \\n{losses}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLRp9_S8Qsy0",
        "outputId": "19bfd98d-8dac-4b05-f1c4-34ff9a2d4319"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Losses: \n",
            "[2.12723555 2.11671634 2.10645047 2.09644267 2.0866964  2.07721384\n",
            " 2.06799602 2.05904287 2.05035331 2.04192534]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall the Gradient descent with momentum update rule:\n",
        "$$\n",
        "\\begin{align}\n",
        "v^{+} =& \\beta v + (1-\\beta) \\nabla L(v)\\\\\n",
        "w^{+} =& w - \\eta v^{+}\n",
        "\\end{align}\n",
        "$$\n",
        "where $\\beta$ is the momentum factor and $\\eta$ is the learning rate.\n",
        "\n",
        "Using the same strategy, we can implement the `GradientDescentWithMomentum` optimizer.\n",
        "\n",
        "**Exercise 8**\n",
        "1. Unlike plain gradient descent, `GradientDescentWithMomentum` requires an additional input: the momentum factor `beta`. So, we will redefine the `__init__()` method to include it.\n",
        "2. Implement the `step()` method by using GD with momentum formula"
      ],
      "metadata": {
        "id": "OeTWN_f0RUBT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GradientDescentWithMomentum(Optimizer):\n",
        "    def __init__(self, network, learning_rate, beta=0.9):\n",
        "        super().__init__(network, learning_rate)\n",
        "        self.beta = beta\n",
        "        self.velocities = {}\n",
        "\n",
        "    def step(self):\n",
        "        for key in self.network.parameters.keys():\n",
        "            # Get corresponding gradient key: W1 -> dW1, b1 -> db1\n",
        "            grad_key = 'd' + key\n",
        "\n",
        "            # Get the parameter value and gradient value\n",
        "            param = self.network.parameters[key]\n",
        "            grad = self.network.grads[grad_key]\n",
        "\n",
        "            # Initialize velocity if not present\n",
        "            if key not in self.velocities:\n",
        "                self.velocities[key] = np.zeros_like(param)\n",
        "\n",
        "            # Get the previous velocity for this parameter\n",
        "            velocity = self.velocities[key]\n",
        "\n",
        "            # Update the velocity and parameter using the momentum formula\n",
        "            ### Code here ### (~ 2 lines of code)\n",
        "\n",
        "\n",
        "            ### Code here ### (~ 2 lines of code)\n",
        "\n",
        "            # Store the updated velocity and parameter back into the neural network\n",
        "            self.velocities[key] = velocity\n",
        "            self.network.parameters[key] = param\n"
      ],
      "metadata": {
        "id": "cijDYEV5SFbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = GradientDescentWithMomentum(network, learning_rate = 0.1, beta=0.8)\n",
        "\n",
        "max_iter = 10\n",
        "losses = []\n",
        "for i in range(max_iter):\n",
        "    A = network(X)\n",
        "    network.backward(Y)\n",
        "    losses.append(compute_cost(A, Y))\n",
        "    optimizer.step()\n",
        "\n",
        "losses = np.array(losses)\n",
        "print(f\"Losses: \\n{losses}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99UZxpNmVmpr",
        "outputId": "e6a4ebaa-d4e0-48f3-b653-4148e338d27b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Losses: \n",
            "[2.03375613 2.03216316 2.02931359 2.02548826 2.02091931 2.01579704\n",
            " 2.01027639 2.00448266 1.99851656 1.99245847]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 - RMSProp\n",
        "\n",
        "As introduced in the lectures, we can also scale the gradient coordinates to obtain faster convergence. One of the commonly used method is `RMSProp`. The update rule is given by\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "s^{+} =& \\beta s + (1-\\beta) g^2\\\\\n",
        "w^{+} =& w - \\eta g/ \\sqrt{s^{+}}\n",
        "\\end{align}\n",
        "$$\n",
        "where $\\beta$ is the scaling factor.\n",
        "\n",
        "\n",
        "Using the same strategy we can implement the `RMSProp` optimizer.\n",
        "\n",
        "**Exercise 9**\n",
        "1. Like `GradientDescentWithMomentum` requires an additional input, the `RMSProp` also require a scaling factor `beta`, so we also need to redefine the `__init__()` method\n",
        "2. Implement the `step()` method by using RMSProp formula"
      ],
      "metadata": {
        "id": "UXEmx7WqV2oF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RMSProp(Optimizer):\n",
        "    def __init__(self, network, learning_rate, beta=0.9):\n",
        "        super().__init__(network, learning_rate)\n",
        "        self.beta = beta\n",
        "        self.squared_gradients = {}\n",
        "\n",
        "    def step(self):\n",
        "        for key in self.network.parameters.keys():\n",
        "            # Get corresponding gradient key: W1 -> dW1, b1 -> db1\n",
        "            grad_key = 'd' + key\n",
        "\n",
        "            # Get the parameter value and gradient value\n",
        "            param = self.network.parameters[key]\n",
        "            grad = self.network.grads[grad_key]\n",
        "\n",
        "            # Initialize running average of squared gradients if not present\n",
        "            if key not in self.squared_gradients:\n",
        "                self.squared_gradients[key] = np.zeros_like(param)\n",
        "\n",
        "            # Get the previous squared_gradient for this parameter\n",
        "            squared_gradient = self.squared_gradients[key]\n",
        "\n",
        "            # Update the squared_gradient and parameter using the RMSProp formula\n",
        "            ### Code here ### (~ 2 lines of code)\n",
        "\n",
        "\n",
        "            ### Code here ### (~ 2 lines of code)\n",
        "\n",
        "            # Store the updated parameter back into the neural network\n",
        "            self.squared_gradients[key] = squared_gradient\n",
        "            self.network.parameters[key] = param\n",
        "\n"
      ],
      "metadata": {
        "id": "Q80XO3ofV2BL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test RMSProp\n",
        "optimizer = RMSProp(network, learning_rate = 0.1, beta=0.8)\n",
        "\n",
        "max_iter = 10\n",
        "losses = []\n",
        "for i in range(max_iter):\n",
        "    A = network(X)\n",
        "    network.backward(Y)\n",
        "    losses.append(compute_cost(A, Y))\n",
        "    optimizer.step()\n",
        "\n",
        "losses = np.array(losses)\n",
        "print(f\"Losses: \\n{losses}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZ9GQ8pHXKGe",
        "outputId": "d9ac42e0-6b0b-42eb-e766-654225ef374a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Losses: \n",
            "[1.98637207 1.81473496 1.77935884 1.75574723 1.73123266 1.69885805\n",
            " 1.65468703 1.59739508 1.52794071 1.45044982]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4 -  Adam\n",
        "\n",
        "Using the same strategy we can implement the `Adam` optimizer. The update rules for `Adam` are:\n",
        "$$\n",
        "\\begin{align}\n",
        "v^+ = & \\beta_1 v + (1-\\beta_1) g\\\\\n",
        "s^{+} =& \\beta_2 s + (1-\\beta_2) g^2\\\\\n",
        "w^{+} =& w - \\eta v^+/ \\sqrt{s^{+} + \\epsilon }\n",
        "\\end{align}\n",
        "$$\n",
        "where $\\beta_1$ and $\\beta_2$ are momentum and scaling factors, respectively\n",
        "\n",
        "**Exercise 10**\n",
        "1. The `Adam` optimizer requires two factors `beta1` and `beta2`, so we also need to redefine the `__init__()` method\n",
        "2. Implement the `step()` method using Adam formula"
      ],
      "metadata": {
        "id": "YskatLb1XTU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Adam(Optimizer):\n",
        "    def __init__(self, network, learning_rate, beta1=0.9, beta2=0.999):\n",
        "        super().__init__(network, learning_rate)\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.velocities = {}\n",
        "        self.squared_gradients = {}\n",
        "\n",
        "    def step(self):\n",
        "        for key in self.network.parameters.keys():\n",
        "            # Get corresponding gradient key: W1 -> dW1, b1 -> db1\n",
        "            grad_key = 'd' + key\n",
        "\n",
        "            # Get the parameter value and gradient value\n",
        "            param = self.network.parameters[key]\n",
        "            grad = self.network.grads[grad_key]\n",
        "\n",
        "            # Initialize velocity and running average of squared gradients if not present\n",
        "            if key not in self.velocities:\n",
        "              self.velocities[key] = np.zeros_like(param)\n",
        "\n",
        "            if key not in self.squared_gradients:\n",
        "              self.squared_gradients[key] = np.zeros_like(param)\n",
        "\n",
        "            # Get the previous velocity and squared_gradient for this parameter\n",
        "            velocity = self.velocities[key]\n",
        "            squared_gradient = self.squared_gradients[key]\n",
        "\n",
        "            # Update the velocity, squared_gradient, and parameters using the RMSProp formula\n",
        "            ### Code here ### (~ 3 lines of code)\n",
        "\n",
        "\n",
        "\n",
        "            ### Code here ### (~ 3 lines of code)\n",
        "\n",
        "            # Store the updated parameter back into the neural network\n",
        "            self.velocities[key] = velocity\n",
        "            self.squared_gradients[key] = squared_gradient\n",
        "            self.network.parameters[key] = param\n"
      ],
      "metadata": {
        "id": "LkOWKfwrXSo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the Adam optimizer using the training loop\n",
        "optimizer = Adam(network, learning_rate = 0.1, beta1=0.8, beta2=0.9)\n",
        "\n",
        "max_iter = 10\n",
        "losses = []\n",
        "for i in range(max_iter):\n",
        "    A = network(X)\n",
        "    network.backward(Y)\n",
        "    losses.append(compute_cost(A, Y))\n",
        "    optimizer.step()\n",
        "\n",
        "losses = np.array(losses)\n",
        "print(f\"Losses: \\n{losses}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N80VNM4FYxpU",
        "outputId": "7e69614b-8ea3-4d92-ee1c-0eb79b3eb997"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Losses: \n",
            "[1.3747617  1.3401749  1.30033804 1.26212976 1.22939172 1.20369176\n",
            " 1.18448922 1.17062461 1.16050343 1.15281649]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 - Training with Optimizers\n",
        "\n",
        "We will build a general `train()` function that takes a `model`, an `optimizer`, along with `X_train` and `Y_train`, and returns the losses. We can then plot the losses to observe the convergence behavior.\n",
        "\n",
        "Additionally, we will test the trained model on `X_test` and `Y_test`. To improve code efficiency, we will create a `train_loop()` function that takes `model`, `optimizer`, `X`, `Y`, and a boolean flag `evaluate`.\n",
        "\n",
        "- If `evaluate=False`, the `train_loop()` performs a training step: the model runs `backward()` to compute gradients and `optimizer.step()` to update the parameters.\n",
        "- If `evaluate=True`, the `train_loop()` evaluates the model on `X` and `Y` without performing backpropagation or updating the parameters.\n"
      ],
      "metadata": {
        "id": "DqwjKKS1GfJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(model, optimizer, X, Y, evaluate=False, print_cost=False):\n",
        "    outputs = model(X)\n",
        "    loss = compute_cost(outputs, Y)\n",
        "    if not evaluate:\n",
        "        model.backward(Y)\n",
        "        optimizer.step()\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "3_aOYmdQG_7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, optimizer, X_train, Y_train, X_test, Y_test, num_iterations=10, print_cost=False):\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "        # Perform training and evaluation every iteration\n",
        "        test_loss = train_loop(model, optimizer, X_test, Y_test, evaluate=True, print_cost=print_cost)\n",
        "        train_loss = train_loop(model, optimizer, X_train, Y_train, evaluate=False, print_cost=print_cost)\n",
        "\n",
        "        # Store both train and test losses\n",
        "        train_losses.append(train_loss)\n",
        "        test_losses.append(test_loss)\n",
        "\n",
        "        # Optionally print the loss values\n",
        "        if print_cost and i % 10 == 0:\n",
        "            print(f\"Train Loss at {i}: {train_loss}; Test Loss at {i}: {test_loss}\")\n",
        "\n",
        "    return np.array(train_losses), np.array(test_losses)"
      ],
      "metadata": {
        "id": "2y6-akYOZNKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "network = NeuralNetwork(n_x, n_y, n_h, depth=3)\n",
        "optimizer = Adam(network, learning_rate = 0.1, beta1=0.8, beta2=0.9)\n",
        "train_losses, test_losses = train(network, optimizer, X, Y, X, Y, num_iterations=10)\n",
        "print(f\"Train Losses: \\n{train_losses}\")\n",
        "print(f\"Test Losses: \\n{test_losses}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzG6G9WpMUe7",
        "outputId": "ef4aec4e-bc57-4f15-f8f6-506e40f3ecea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Losses: \n",
            "[1.8452532  1.80712141 1.74078336 1.75232359 1.72123482 1.7096015\n",
            " 1.70552052 1.68568429 1.5995576  1.52332713]\n",
            "Test Losses: \n",
            "[1.8452532  1.80712141 1.74078336 1.75232359 1.72123482 1.7096015\n",
            " 1.70552052 1.68568429 1.5995576  1.52332713]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 - Load Image Dataset: MNIST"
      ],
      "metadata": {
        "id": "8pVdp6dak3L6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us test your DNN model on the MNIST dataset, which contains images of digits `0` through `9`. For simplicity, we will select only the digits `0` and `1` for binary classification."
      ],
      "metadata": {
        "id": "zS074a2RMjxz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Flatten the 28x28 images into vectors of 784 elements and normalize to [0, 1]\n",
        "X_train = X_train.reshape(X_train.shape[0], -1).T / 255.0  # Transpose to (in_features, num_samples)\n",
        "X_test = X_test.reshape(X_test.shape[0], -1).T / 255.0     # Transpose to (in_features, num_samples)\n",
        "\n",
        "# Select only the samples of class '0' and '1' for binary classification\n",
        "train_filter = (y_train == 0) | (y_train == 1)\n",
        "test_filter = (y_test == 0) | (y_test == 1)\n",
        "\n",
        "X_train_binary = X_train[:, train_filter]\n",
        "y_train_binary = y_train[train_filter].reshape(1, -1)  # Reshape to (1, num_samples)\n",
        "\n",
        "X_test_binary = X_test[:, test_filter]\n",
        "y_test_binary = y_test[test_filter].reshape(1, -1)  # Reshape to (1, num_samples)\n",
        "\n",
        "# Verify the shapes\n",
        "print(f\"Training data shape: {X_train_binary.shape}\")  # Should be (784, num_samples)\n",
        "print(f\"Training labels shape: {y_train_binary.shape}\")  # Should be (1, num_samples)\n",
        "print(f\"Testing data shape: {X_test_binary.shape}\")  # Should be (784, num_samples)\n",
        "print(f\"Testing labels shape: {y_test_binary.shape}\")  # Should be (1, num_samples)\n",
        "\n",
        "# Print out some example labels to verify\n",
        "print(\"Training labels:\", np.unique(y_train_binary))\n",
        "print(\"Testing labels:\", np.unique(y_test_binary))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZG3M3A_PMpI8",
        "outputId": "7214c3b3-ac3e-4c30-da79-2f10f90e401f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape: (784, 12665)\n",
            "Training labels shape: (1, 12665)\n",
            "Testing data shape: (784, 2115)\n",
            "Testing labels shape: (1, 2115)\n",
            "Training labels: [0 1]\n",
            "Testing labels: [0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Optimizer Comparison\n",
        "\n",
        "In this exercise, we will test different optimization algorithms on a simple neural network trained for binary classification using the MNIST dataset (with digits ‘0’ and ‘1’). The code initializes a neural network with a specified number of input neurons, hidden neurons, output neurons, and depth.\n",
        "\n",
        "We then experiment with different optimization algorithms, including Gradient Descent and Momentum, to observe their effects on the model’s performance. The code runs each optimizer for a specified number of iterations and records the training and test losses.\n",
        "\n",
        "The results are plotted to compare how each optimizer influences the convergence of the model. This will help you understand the differences in performance between the optimizers and how they affect the model’s ability to generalize to new data."
      ],
      "metadata": {
        "id": "ubqhTcjWk0AS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize neural network and optimizer settings\n",
        "input_size = X_train_binary.shape[0]\n",
        "hidden_size = 64\n",
        "output_size = 1\n",
        "depth = 3\n",
        "num_iterations = 100\n",
        "\n",
        "# Dictionary of optimizers and their settings\n",
        "optimizers = {\n",
        "    \"Gradient Descent\": lambda net: GradientDescent(net, learning_rate=0.1),\n",
        "    # \"Momentum\": lambda net: GradientDescentWithMomentum(net, learning_rate=0.9, beta=0.9),\n",
        "    \"RMSProp\": lambda net: RMSProp(net, learning_rate=0.001, beta=0.9),\n",
        "    # \"Adam\": lambda net: Adam(net, learning_rate=0.01, beta1=0.9, beta2=0.99)\n",
        "}\n",
        "\n",
        "# Define colors for each optimizer\n",
        "colors = {\n",
        "    \"Gradient Descent\": 'b',\n",
        "    # \"Momentum\": 'k',\n",
        "    \"RMSProp\": 'r',\n",
        "    # \"Adam\": 'c'\n",
        "}\n",
        "\n",
        "# Dictionary to store losses for each optimizer\n",
        "train_losses_dict = {}\n",
        "test_losses_dict = {}\n",
        "\n",
        "# Loop through each optimizer, reinitialize the network, and train\n",
        "for opt_name, optimizer_fn in optimizers.items():\n",
        "    print(f\"Training with {opt_name} ...\")\n",
        "    np.random.seed(1)\n",
        "    mnist_net = NeuralNetwork(input_size, output_size, hidden_size, depth)\n",
        "    optimizer = optimizer_fn(mnist_net)\n",
        "    train_losses, test_losses = train(mnist_net, optimizer, X_train_binary, y_train_binary, X_test_binary, y_test_binary, num_iterations=num_iterations)\n",
        "    train_losses_dict[opt_name] = train_losses\n",
        "    test_losses_dict[opt_name] = test_losses\n",
        "\n",
        "    print(f\"Train Losses: {train_losses[-1]:.4f}, Test Losses: {test_losses[-1]:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "# Plot both training and test losses in the same figure with the same color for each optimizer\n",
        "plt.figure(figsize=(10, 6))\n",
        "for opt_name in optimizers.keys():\n",
        "    color = colors[opt_name]\n",
        "    plt.plot(train_losses_dict[opt_name], label=f'{opt_name} Train Loss', linestyle='-', color=color)\n",
        "    plt.plot(test_losses_dict[opt_name], label=f'{opt_name} Test Loss', linestyle='--', color=color)\n",
        "\n",
        "plt.title(\"Training and Test Loss Comparison\")\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        },
        "id": "zDvWqma4DGDL",
        "outputId": "95a029e0-4334-484e-f609-60d5b15b57f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with Gradient Descent ...\n",
            "Train Losses: 0.0033, Test Losses: 0.0029\n",
            "Training with RMSProp ...\n",
            "Train Losses: 0.0035, Test Losses: 0.0041\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Congratulations\n",
        "\n",
        "on completing the assignment! You’ve successfully implemented a flexible deep neural network with customizable width and depth, along with advanced optimization algorithms such as Gradient Descent, Momentum, RMSProp and Adam. By exploring these optimizers, you’ve gained valuable insights into their impact on training efficiency and performance. Great work—keep pushing forward as you continue to master deep learning techniques!"
      ],
      "metadata": {
        "id": "wY0cmTmomEJy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NYysOXdHcv5H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}