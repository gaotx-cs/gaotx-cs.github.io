---
layout: course
course_code: CSC483
title: Introduction to Deep Learning
semester: Winter 2026, Fall 2025
instructor: Tianxiang Gao
instructor_url: https://gaotx-cs.github.io/
time: Thursdays 5:45PM - 9:00PM
location: CDM Center 224 at Loop Campus
office_hours: Mondays 10:00AM-11:00AM
zoom: https://depaul.zoom.us/my/gaotx
discussion: https://discord.gg/2xncA2rATU
description: >
  This course covers the foundations of deep learning, including fundamental neural network architectures (e.g., multilayer perceptrons) and training methodologies, including widely used optimization techniques (e.g., momentum, RMSprop, Adam). It also addresses generalization and regularization strategies (e.g., overparameterization, the double descent phenomenon, and weight decay). We will explore cutting-edge neural network architectures, including convolutional neural networks (CNNs), recurrent neural networks (RNNs), and transformers (e.g., GPT and BERT) with attention mechanisms. Students will gain hands-on experience by implementing these models and applying them to real-world problems in computer vision (CV), natural language processing (NLP), and computational biology. 

  This course covers the foundations of deep learning, including fundamental neural network architectures (e.g., multilayer perceptrons) and training methodologies, including advanced optimization techniques (e.g., momentum, RMSprop, Adam). It also addresses generalization and regularization strategies (e.g., overparameterization, the double descent phenomenon, and weight decay). We will explore cutting-edge neural network architectures, including convolutional neural networks (CNNs), recurrent neural networks (RNNs), transformers (e.g., GPT and BERT), and graph neural networks (GNNs). Students will gain hands-on experience by implementing these models and applying them to real-world problems in computer vision, natural language processing, and graph machine learning.

syllabus: /assets/pdf/teaching/csc483/syallubs.pdf
# ta: Mohammed Azeezulla 
# ta_email: mmoha134@depaul.edu
logistics: /assets/pdf/teaching/csc483/00_logistics.pdf
# materials:
#   - title: Lecture on Deep Learning
#     type: slides
#     url: /assets/pdf/teaching/01_intro.pdf
#   - title: Code for Deep Learning
#     type: code
#     url: /assets/pdf/teaching/01_intro.pdf.zip
---


## Prerequisites
- [CSC 412](https://www.cdm.depaul.edu/academics/pages/courseinfo.aspx?Subject=CSC&CatalogNbr=412) provides basic knowledge in linear algebra, multivariate calculus, and probability.
- [CSC 480](https://www.cdm.depaul.edu/academics/pages/courseinfo.aspx?CrseId=001513) introduces the fundamental concepts of artificial intelligence and machine learning.

---
## Textbook
No textbook is required. Materials will be drawn from classical books and recent papers. Recommended readings:  

- [Dive into Deep Learning](https://d2l.ai/) by Aston Zhang, Zack C. Lipton, Mu Li, Alex J. Smola. 
- [Deep Learning book](https://www.deeplearningbook.org/) by Goodfellow, Bengio, and Courville  

A list of key papers in deep learning will also be provided.

---
## Grading
- **Quizzes:** 25%  
- **Programming Assignments:** 30%  
- **Student-Designed Assignment :** 25%  
- **Peer-Reviewed Evaluation:** 20% 
- **Bonus Points (Optional): 5%**

---
## Schedule

- **Week 1: Introduction to Neural Networks**. [Slides](/assets/pdf/teaching/csc483/01_intro.pdf) 
  - [Deep Learning](https://www.nature.com/articles/nature14539), *Nature 2015*  

- **Week 2: Training Neural Networks**. [Slides](#)
  - [Back-propagation](https://www.nature.com/articles/323533a0), *Nature 1986*  
  - [Understanding the difficulty of training DNNs](https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf), *AISTAT 2010*  
  - [On the difficulty of training RNNs](https://proceedings.mlr.press/v28/pascanu13.html), *ICML 2013*  
  - [He initialization](https://proceedings.mlr.press/v28/pascanu13.html), *ICCV 2015*  


---
## Additional Reading and Resources
- [Review of Linear Algebra](/assets/pdf/teaching/cs229-linalg.pdf), *by Zico Kolter and Chuong Do from Stanford*
- [Review of Probability Theory](/assets/pdf/teaching/cs229-prob.pdf), *by Arian Maleki and Tom Do from Stanford* 
- [11-785 Introduction to Deep Learning](https://deeplearning.cs.cmu.edu/), *by Bhiksha Raj and Rita Singh at CMU, Fall 2024*
- [Deep Learning Specialization](), *by Andrew Ng at Coursera and DeepLearning.AI, Fall 2021*
- [Foundations of Machine Learning](https://cs.nyu.edu/~mohri/mlbook/), *Textbook by Mehryar Mohri, 2018* 
- [Towards Understanding the Role of Over-Parametrization in Generalization of Neural Networks](https://openreview.net/forum?id=BygfghAcYX), *ICLR 2019*
- [Deep Networks with Stochastic Depth](https://arxiv.org/abs/1603.09382), *ECCV 2016*
- [Deep Double Descent: Where Bigger Models and More Data Hurt](https://openreview.net/forum?id=B1g5sA4twr), *ICLR 2020*
- [Improving Language Understanding by Generative Pre-Training](https://openai.com/index/language-unsupervised/), *OpenAI Blog 2018 (Introduced GPT-1)* 
- [Language Models are Unsupervised Multitask Learners](https://openai.com/index/better-language-models/), *OpenAI Blog 2019 (Introduced GPT-2)* 
- [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556), *NeurIPS 2022*
- [The Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/), *Harvard NLP Blog 2018*


---
## Assignments

