---
layout: course
course_code: CSC578
title: Neural Networks and Deep Learning
semester: Spring 2025, Winter 2025, Fall 2024
instructor: Tianxiang Gao
instructor_url: https://gaotx-cs.github.io/
time: Thursdays 5:45PM - 9:00PM
location: CDM Center 224 at Loop Campus
office_hours: Mondays 9:00AM-11:00AM
zoom: https://depaul.zoom.us/my/gaotx
discussion: https://discord.gg/JxVhZw8ZWB
description: >
  This course covers the foundations of deep learning, including fundamental neural network architectures (e.g., multilayer perceptrons) and training methodologies, including advanced optimization techniques (e.g., momentum, RMSprop, Adam). It also addresses generalization and regularization strategies (e.g., overparameterization, the double descent phenomenon, and weight decay). We will explore cutting-edge neural network architectures, including convolutional neural networks (CNNs), recurrent neural networks (RNNs), transformers (e.g., GPT and BERT), and graph neural networks (GNNs). Students will gain hands-on experience by implementing these models and applying them to real-world problems in computer vision, natural language processing, and graph machine learning.

syllabus: /assets/pdf/teaching/csc578/csc578_syallubs.pdf
# ta: Mohammed Azeezulla 
# ta_email: mmoha134@depaul.edu
logistics: /assets/pdf/teaching/csc578/00_logistics.pdf
# materials:
#   - title: Lecture on Deep Learning
#     type: slides
#     url: /assets/pdf/teaching/01_intro.pdf
#   - title: Code for Deep Learning
#     type: code
#     url: /assets/pdf/teaching/01_intro.pdf.zip
---


## Prerequisites
- [CSC 412](https://www.cdm.depaul.edu/academics/pages/courseinfo.aspx?Subject=CSC&CatalogNbr=412) provides basic knowledge in linear algebra, multivariate calculus, and probability.
- [DSC 478](https://www.cdm.depaul.edu/academics/pages/courseinfo.aspx?CrseId=012551) or [CSC 480](https://www.cdm.depaul.edu/academics/pages/courseinfo.aspx?CrseId=001513) introduces the fundamental concepts of artificial intelligence and machine learning.
- You will implement and train deep neural networks using PyTorch, so basic Python proficiency is required.

---
## Textbook
No textbook is required. Materials will be drawn from classical books and recent papers. Recommended readings:  

- [Dive into Deep Learning](https://d2l.ai/) by Aston Zhang, Zack C. Lipton, Mu Li, Alex J. Smola. 
- [Deep Learning book](https://www.deeplearningbook.org/) by Goodfellow, Bengio, and Courville  

A list of key papers in deep learning will also be provided.

---
## Grading
- **Quizzes:** 25%  
- **Programming Assignments:** 35%  
- **Midterm:** 20%  
- **Final Project:** 20% (Proposal: 8%, Final Report: 12%)  

Only the best **5** out of 10 quizzes and assignments will count toward the final grade.

---
## Schedule

- **Week 1: Introduction to Neural Networks**. [Slides](/assets/pdf/teaching/csc578/01_intro.pdf), [Video](https://courseonline.cdm.depaul.edu/courseplayer/courses/64511/lecture/485761) 
  - [Deep Learning](https://www.nature.com/articles/nature14539), *Nature 2015*  

- **Week 2: Training Neural Networks**. [Slides](/assets/pdf/teaching/csc578/02_train.pdf), [Video](https://courseonline.cdm.depaul.edu/courseplayer/courses/64511/lecture/485762)  
  - [Back-propagation](https://www.nature.com/articles/323533a0), *Nature 1986*  
  - [Understanding the difficulty of training DNNs](https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf), *AISTAT 2010*  
  - [On the difficulty of training RNNs](https://proceedings.mlr.press/v28/pascanu13.html), *ICML 2013*  
  - [He initialization](https://proceedings.mlr.press/v28/pascanu13.html), *ICCV 2015*  

- **Week 3: Advanced Optimizers**. [Slides](/assets/pdf/teaching/csc578/03_opt.pdf), [Video](https://courseonline.cdm.depaul.edu/courseplayer/courses/64511/lecture/485763)  
  - [On the Importance of Initialization and Momentum in Deep Learning](https://proceedings.mlr.press/v28/sutskever13.html), *ICML 2013*  
  - [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980), *ICLR 2017*  
  - [RMSProp: Divide the Gradient by a Running Average of Its Recent Magnitude](https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf), *Lecture Slice by Geoff Hinton, 2012*  
  - [Stochastic Gradient Descent](https://www.stat.cmu.edu/~ryantibs/convexopt/lectures/stochastic-gd.pdf), *Lecture Slice by Ryan Tibshirani at CMU, Fall 2019*  

- **Week 4: Generalization and Regularization**. [Slides](/assets/pdf/teaching/csc578/04_Gen.pdf), [Video](#)  
  - [Understanding Deep Learning Requires Rethinking Generalization](https://arxiv.org/abs/1611.03530), *ICLR 2017*  
  - [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](https://jmlr.org/papers/v15/srivastava14a.html), *JMLR 2015*
  - [Averaging Weights Leads to Wider Optima and Better Generalization](https://arxiv.org/abs/1803.05407), *UAI 2018*
  - [On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima](https://arxiv.org/abs/1609.04836), *ICLR 2017*
  - [SGDR: Stochastic Gradient Descent with Warm Restarts](https://openreview.net/forum?id=Skq89Scxx), *ICLR 2017*
  - [Reconciling modern machine learning practice and the bias-variance trade-off](https://arxiv.org/abs/1812.11118), *PNAS 2019*

- **Week 5: CNNs**. [Slides](/assets/pdf/teaching/csc578/05_cnn.pdf), [Video](#)  
  - [ImageNet Classification with Deep Convolutional Neural Networks](https://dl.acm.org/doi/10.1145/3065386), *NeurIPS 2012*  
  - [U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597), *MICCAI 2015*
  - [Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations](https://dl.acm.org/doi/abs/10.1145/1553374.1553453), *ICML 2009* 
  - [Very deep convolutional networks for large-scale image recognition](https://arxiv.org/abs/1409.1556), *ICLR 2015*
  - [Deep residual learning for image recognition](https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf), *CVPR 2016*
  - [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167), *ICML 2015*

- **Week 6: Learning with CNNs**. [Slides](/assets/pdf/teaching/csc578/06_cv.pdf), [Video](#)  
  - [Network in Network](https://arxiv.org/abs/1312.4400), *ICLR 2014*
  - [Going Deeper with Convolutions](https://arxiv.org/abs/1409.4842), *CVPR 2014*
  - [MobileNetV2: Inverted Residuals and Linear Bottlenecks](https://arxiv.org/abs/1801.04381v4), *CVPR 2018* 
  - [EfficientNet: Rethinking Model Scaling for CNNs](https://arxiv.org/abs/1905.11946), *ICML 2019*
  - [You Only Look Once: Unified, Real-Time Object Detection](https://arxiv.org/abs/1502.03167), *CVPR 2016*
  - [DeepFace: Closing the Gap to Human-Level Performance in Face Verification](https://ieeexplore.ieee.org/document/6909616), *CVPR 2014*
  - [FaceNet: A Unified Embedding for Face Recognition and Clustering](https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Schroff_FaceNet_A_Unified_2015_CVPR_paper.html), *CVPR 2015*
  - [Visualizing and Understanding Convolutional Networks](https://arxiv.org/abs/1311.2901), *ECCV 2014*

- **Week 7: RNNs**. [Slides](/assets/pdf/teaching/csc578/07_rnn.pdf), [Video](#)  
  - [Learning to Forget: Continual Prediction with LSTM](https://www.sciencedirect.com/science/article/abs/pii/S0893608005001206), *Neural Computation 2005*  
  - [Learning Phrase Representations using RNN Encoderâ€“Decoder for Statistical Machine Translation](https://arxiv.org/pdf/1406.1078), *EMNL 2014*
  - [Bidirectional Recurrent Neural Networks](https://ieeexplore.ieee.org/abstract/document/650093), *IEEE TSP 1997*
  - [Linguistic Regularities in Continuous Space Word Representations](https://aclanthology.org/N13-1090.pdf), *NAACL 2013*
  - [A Neural Probabilistic Language Model](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf), *JMLR 2003*
  - [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781), *ICLR 2013*
  - [GloVe: Global Vectors for Word Representation](https://aclanthology.org/D14-1162.pdf), *EMNLP 2014*

- **Week 8: Seq2Seq Models and Transformers**. [Slides](/assets/pdf/teaching/csc578/08_seq2seq.pdf), [Video](#)  
  - [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/pdf/1406.1078), *EMNLP 2014*
  - [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215), *NeurIPS 2014*  
  - [BLEU: a Method for Automatic Evaluation of Machine Translation](https://proceedings.neurips.cc/paper_files/paper/2016/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf), *ACL 2002*
  - [Neural Machine Translation by Jointly Learning to Align and Translate](https://papers.baulab.info/papers/Bahdanau-2015.pdf), *ICLR 2015*
  - [Attention Is All You Need](https://arxiv.org/abs/1706.03762), *NeurIPS 2017*

- **Week 9: LLMs and Efficient Transformers**. [Slides](/assets/pdf/teaching/csc578/09_llm.pdf), [Video](#)  
  - [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805), *NAACL 2019*
  - [Language Models are Few-Shot Learners](https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf), *NeurIPS 2020 (Introduced GPT-3)*
  - [Scaling Laws for Neural Language Models](https://openai.com/index/scaling-laws-for-neural-language-models/), *OpenAI Blog 2020*
  - [Training Language Models to Follow Instructions with Human Feedback](https://arxiv.org/abs/2203.02155), *NeurIPS 2022*

- **Week 10: GNNs**. [Slides](/assets/pdf/teaching/csc578/10_gnn.pdf), [Video](#)  
  - [Semi-Supervised Classification with Graph Convolutional Networks](https://arxiv.org/abs/1609.02907), *ICLR 2017*  
  - [Neural Message Passing for Quantum Chemistry](https://arxiv.org/abs/1704.01212), *ICML 2017*
  - [Graph Attention Networks](https://arxiv.org/abs/1710.10903), *ICLR 2018*
  - [How Powerful are Graph Neural Networks?](https://arxiv.org/abs/1810.00826), *ICLR 2019*
  - [Recipe for a General, Powerful, Scalable Graph Transformer](https://arxiv.org/abs/2205.12454), *NeurIPS 2022*

---
## Additional Reading and Resources
- [Review of Linear Algebra](/assets/pdf/teaching/cs229-linalg.pdf), *by Zico Kolter and Chuong Do from Stanford*
- [Review of Probability Theory](/assets/pdf/teaching/cs229-prob.pdf), *by Arian Maleki and Tom Do from Stanford* 
- [10-725 Convex Optimization Course Notes](https://www.stat.cmu.edu/~ryantibs/convexopt/), *by Ryan Tibshirani at CMU, Fall 2019*  
- [11-785 Introduction to Deep Learning](https://deeplearning.cs.cmu.edu/), *by Bhiksha Raj and Rita Singh at CMU, Fall 2024*
- [Deep Learning Specialization](), *by Andrew Ng at Coursera and DeepLearning.AI, Fall 2021*
- [Foundations of Machine Learning](https://cs.nyu.edu/~mohri/mlbook/), *Textbook by Mehryar Mohri, 2018* 
- [Lectures on Convex Optimization](https://link.springer.com/book/10.1007/978-3-319-91578-4), *Textbook by Yurii Nesterov, 2018*  
- [Towards Understanding the Role of Over-Parametrization in Generalization of Neural Networks](https://openreview.net/forum?id=BygfghAcYX), *ICLR 2019*
- [Deep Networks with Stochastic Depth](https://arxiv.org/abs/1603.09382), *ECCV 2016*
- [Deep Double Descent: Where Bigger Models and More Data Hurt](https://openreview.net/forum?id=B1g5sA4twr), *ICLR 2020*
- [Speech recognition with deep recurrent neural networks](https://arxiv.org/abs/1303.5778), *ICASSP 2013*
- [Man is to computer programmer as woman is to homemaker? Debiasing word embeddings](https://proceedings.neurips.cc/paper_files/paper/2016/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf), *NeurIPS 2016*
- [Show and Tell: A Neural Image Caption Generator](https://arxiv.org/abs/1411.4555), *CVPR 2015*
- [Improving Language Understanding by Generative Pre-Training](https://openai.com/index/language-unsupervised/), *OpenAI Blog 2018 (Introduced GPT-1)* 
- [Language Models are Unsupervised Multitask Learners](https://openai.com/index/better-language-models/), *OpenAI Blog 2019 (Introduced GPT-2)* 
- [Emergent Abilities of Large Language Models](https://openreview.net/forum?id=yzkSU5zdwD&utm_source=substack&utm_medium=email), *TMLR 2022*
- [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556), *NeurIPS 2022*
- [Deep Reinforcement Learning from Human Preferences](https://arxiv.org/abs/1706.03741), *NeurIPS 2017*
- [The Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/), *Harvard NLP Blog 2018*
- [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/abs/2101.03961), *JMLR 2022*
- [Predict Then Propagate: Graph Neural Networks Meet Personalized PageRank](https://arxiv.org/abs/1810.05997), *ICLR 2018*


---
## Assignments
- **Assignment 1: Python Basics and Multilayer Perceptron**  
  [Download Notebook](/assets/jupyter/csc578/HW01.ipynb) | [Open in Colab](https://colab.research.google.com/github/gaotx-cs/gaotx-cs.github.io/blob/main/assets/jupyter/csc578/HW01.ipynb)

- **Assignment 2: Neural Network Training: MLP, Backpropogation, Gradient Descent**  
  [Download Notebook](/assets/jupyter/csc578/HW02.ipynb) | [Open in Colab](https://colab.research.google.com/github/gaotx-cs/gaotx-cs.github.io/blob/main/assets/jupyter/csc578/HW02.ipynb)

- **Assignment 3: Advanced Optimizers: Accelerated GD, RMSProp, Adam**  
  [Download Notebook](/assets/jupyter/csc578/HW03.ipynb) | [Open in Colab](https://colab.research.google.com/github/gaotx-cs/gaotx-cs.github.io/blob/main/assets/jupyter/csc578/HW03.ipynb)

- **Assignment 4: Generalization and Regularization: PyTorch, Autograd, Hyperparameter Tune, Overparameterization**  
  [Download Notebook](/assets/jupyter/csc578/HW04.ipynb) | [Open in Colab](https://colab.research.google.com/github/gaotx-cs/gaotx-cs.github.io/blob/main/assets/jupyter/csc578/HW04.ipynb)

- **Assignment 5: Introduction to CNNs: Implementation of CNNs, Semantic Segmentation through UNet**  
  [Download Notebook](/assets/jupyter/csc578/HW05.ipynb) | [Open in Colab](https://colab.research.google.com/github/gaotx-cs/gaotx-cs.github.io/blob/main/assets/jupyter/csc578/HW05.ipynb)

- **Assignment 6: Computer Vision with CNNs: Neural Style Transfer using Pre-Trained VGG**  
  [Download Notebook](/assets/jupyter/csc578/HW06.ipynb) | [Open in Colab](https://colab.research.google.com/github/gaotx-cs/gaotx-cs.github.io/blob/main/assets/jupyter/csc578/HW06.ipynb)

- **Assignment 7: Recurrent Neural Networks: Implementation and Training**  
  [Download Notebook](/assets/jupyter/csc578/HW07.ipynb) | [Open in Colab](https://colab.research.google.com/github/gaotx-cs/gaotx-cs.github.io/blob/main/assets/jupyter/csc578/HW07.ipynb)

- **Assignment 8: Seq2seq: Neural Machine Translation**  
  [Download Notebook](/assets/jupyter/csc578/HW08.ipynb) | [Open in Colab](https://colab.research.google.com/github/gaotx-cs/gaotx-cs.github.io/blob/main/assets/jupyter/csc578/HW08.ipynb)

- **Assignment 9: Transformer: GPT and ChatBot**  
  [Download Notebook](/assets/jupyter/csc578/HW09.ipynb) | [Open in Colab](https://colab.research.google.com/github/gaotx-cs/gaotx-cs.github.io/blob/main/assets/jupyter/csc578/HW09.ipynb)
