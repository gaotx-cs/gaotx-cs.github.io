---
layout: course
course_code: CSC578
title: Neural Networks and Deep Learning
semester: Spring 2025, Winter 2025, Fall 2024
instructor: Tianxiang Gao
instructor_url: https://gaotx-cs.github.io/
time: Thursdays 5:45PM - 9:00PM
location: CDM Center 224 at Loop Campus
office_hours: Mondays 9:00AM-11:00AM
zoom: https://depaul.zoom.us/my/gaotx
description: >
  This course covers the foundations of deep learning, including fundamental neural network architectures (e.g., multilayer perceptrons) and training methodologies, including advanced optimization techniques (e.g., momentum, RMSprop, Adam). It also addresses generalization and regularization strategies (e.g., overparameterization, the double descent phenomenon, and weight decay). We will explore cutting-edge neural network architectures, including convolutional neural networks (CNNs), recurrent neural networks (RNNs), transformers (e.g., GPT and BERT), and graph neural networks (GNNs). Students will gain hands-on experience by implementing these models and applying them to real-world problems in computer vision, natural language processing, and graph machine learning.

syllabus: /assets/pdf/teaching/csc578/csc578_syallubs.pdf
# ta: Mohammed Azeezulla 
# ta_email: mmoha134@depaul.edu
logistics: /assets/pdf/teaching/csc578/00_logistics.pdf
# materials:
#   - title: Lecture on Deep Learning
#     type: slides
#     url: /assets/pdf/teaching/01_intro.pdf
#   - title: Code for Deep Learning
#     type: code
#     url: /assets/pdf/teaching/01_intro.pdf.zip
---


## Prerequisites
- [CSC 412](https://www.cdm.depaul.edu/academics/pages/courseinfo.aspx?Subject=CSC&CatalogNbr=412) provides basic knowledge in linear algebra, multivariate calculus, and probability.
- [DSC 478](https://www.cdm.depaul.edu/academics/pages/courseinfo.aspx?CrseId=012551) or [CSC 480](https://www.cdm.depaul.edu/academics/pages/courseinfo.aspx?CrseId=001513) introduces the fundamental concepts of artificial intelligence and machine learning.
- You will implement and train deep neural networks using PyTorch, so basic Python proficiency is required.

---
## Textbook
No textbook is required. Materials will be drawn from classical books and recent papers. Recommended readings:  

- [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/) by Michael Nielsen  
- [Deep Learning book](https://www.deeplearningbook.org/) by Goodfellow, Bengio, and Courville  

A list of key papers in deep learning will also be provided.

---
## Grading
- **Quizzes:** 25%  
- **Programming Assignments:** 35%  
- **Midterm:** 20%  
- **Final Project:** 20% (Proposal: 8%, Final Report: 12%)  

Only the best **5** out of 10 quizzes and assignments will count toward the final grade.

---
## Schedule

- **Week 1: Introduction to Neural Networks**. [Slides](/assets/pdf/teaching/csc578/01_intro.pdf), [Video](https://courseonline.cdm.depaul.edu/courseplayer/courses/64511/lecture/485761) 
  - [Deep Learning](https://www.nature.com/articles/nature14539), *Nature 2015*  

- **Week 2: Training Neural Networks**. [Slides](/assets/pdf/teaching/csc578/02_train.pdf), [Video](https://courseonline.cdm.depaul.edu/courseplayer/courses/64511/lecture/485762)  
  - [Back-propagation](https://www.nature.com/articles/323533a0), *Nature 1986*  
  - [Understanding the difficulty of training DNNs](https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf), *AISTAT 2010*  
  - [On the difficulty of training RNNs](https://proceedings.mlr.press/v28/pascanu13.html), *ICML 2013*  
  - [He initialization](https://proceedings.mlr.press/v28/pascanu13.html), *ICCV 2015*  

- **Week 3: Advanced Optimizers**. [Slides](/assets/pdf/teaching/csc578/03_opt.pdf), [Video](https://courseonline.cdm.depaul.edu/courseplayer/courses/64511/lecture/485763)  
  - [On the Importance of Initialization and Momentum in Deep Learning](https://proceedings.mlr.press/v28/sutskever13.html), *ICML 2013*  
  - [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980), *ICLR 2017*  
  - [RMSProp: Divide the Gradient by a Running Average of Its Recent Magnitude](https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf), *Lecture Slice by Geoff Hinton, 2012*  
  - [Stochastic Gradient Descent](https://www.stat.cmu.edu/~ryantibs/convexopt/lectures/stochastic-gd.pdf), *Lecture Slice by Ryan Tibshirani at CMU, Fall 2019*  

- **Week 4: Generalization and Regularization**. [Slides](/assets/pdf/teaching/csc578/04_Gen.pdf), [Video](#)  
  - [Understanding Deep Learning Requires Rethinking Generalization](https://arxiv.org/abs/1611.03530), *ICLR 2017*  
  - [Towards Understanding the Role of Over-Parametrization in Generalization of Neural Networks](https://openreview.net/forum?id=BygfghAcYX), *ICLR 2019*
  - [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](https://jmlr.org/papers/v15/srivastava14a.html), *JMLR 2015*
  - [Averaging Weights Leads to Wider Optima and Better Generalization](https://arxiv.org/abs/1803.05407), *UAI 2018*
  - [Deep Networks with Stochastic Depth](https://arxiv.org/abs/1603.09382), *ECCV 2016*
  - [On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima](https://arxiv.org/abs/1609.04836), *ICLR 2017*
  - [SGDR: Stochastic Gradient Descent with Warm Restarts](https://openreview.net/forum?id=Skq89Scxx), *ICLR 2017*
  - [Reconciling modern machine learning practice and the bias-variance trade-off](https://arxiv.org/abs/1812.11118), *PNAS 2019*
  - [Deep Double Descent: Where Bigger Models and More Data Hurt](https://openreview.net/forum?id=B1g5sA4twr), *ICLR 2020*

- **Week 5: CNNs**. [Slides](/assets/pdf/teaching/csc578/05_cnn.pdf), [Video](#)  
  - [ImageNet Classification with Deep Convolutional Neural Networks](https://dl.acm.org/doi/10.1145/3065386), *NeurIPS 2012*  

- **Week 6: Learning with CNNs**. [Slides](/assets/pdf/teaching/csc578/06_cv.pdf), [Video](#)  
  - [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167), *ICML 2015*  

- **Week 7: RNNs**. [Slides](/assets/pdf/teaching/csc578/07_rnn.pdf), [Video](#)  
  - [Learning to Forget: Continual Prediction with LSTM](https://www.sciencedirect.com/science/article/abs/pii/S0893608005001206), *Neural Computation 2005*  

- **Week 8: Seq2Seq Models**. [Slides](/assets/pdf/teaching/csc578/08_seq2seq.pdf), [Video](#)  
  - [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215), *NeurIPS 2014*  

- **Week 9: Transformers and LLMs**. [Slides](/assets/pdf/teaching/csc578/09_llm.pdf), [Video](#)  
  - [Attention Is All You Need](https://arxiv.org/abs/1706.03762), *NeurIPS 2017*  

- **Week 10: GNNs**. [Slides](/assets/pdf/teaching/csc578/10_gnn.pdf), [Video](#)  
  - [Semi-Supervised Classification with Graph Convolutional Networks](https://arxiv.org/abs/1609.02907), *ICLR 2017*  

---
## Additional Reading and Resources
- [Review of Linear Algebra](/assets/pdf/teaching/cs229-linalg.pdf), *by Zico Kolter and Chuong Do from Stanford*
- [Review of Probability Theory](/assets/pdf/teaching/cs229-prob.pdf), *by Arian Maleki and Tom Do from Stanford* 
- [10-725 Convex Optimization Course Notes](https://www.stat.cmu.edu/~ryantibs/convexopt/), *by Ryan Tibshirani at CMU, Fall 2019*  
- [11-785 Introduction to Deep Learning](https://deeplearning.cs.cmu.edu/), *by Bhiksha Raj and Rita Singh at CMU, Fall 2024*
- [Deep Learning Specialization](), *by Andrew Ng at Coursera and DeepLearning.AI, Fall 2021*
- [Foundations of Machine Learning](https://cs.nyu.edu/~mohri/mlbook/), *Textbook by Mehryar Mohri, 2018* 
- [Lectures on Convex Optimization](https://link.springer.com/book/10.1007/978-3-319-91578-4), *Textbook by Yurii Nesterov, 2018*  

---
## Assignments
- **Assignment 1: Python Basics and Multilayer Perceptron**  
  [Download Notebook](/assets/jupyter/csc578/HW01.ipynb) | [Open in Colab](https://colab.research.google.com/github/gaotx-cs/gaotx-cs.github.io/blob/main/assets/jupyter/csc578/HW01.ipynb)

- **Assignment 2: Neural Network Training: MLP, Backpropogation, Gradient Descent**  
  [Download Notebook](/assets/jupyter/csc578/HW02.ipynb) | [Open in Colab](https://colab.research.google.com/github/gaotx-cs/gaotx-cs.github.io/blob/main/assets/jupyter/csc578/HW02.ipynb)

- **Assignment 3: Advanced Optimizers: Accelerated GD, RMSProp, Adam**  
  [Download Notebook](/assets/jupyter/csc578/HW03.ipynb) | [Open in Colab](https://colab.research.google.com/github/gaotx-cs/gaotx-cs.github.io/blob/main/assets/jupyter/csc578/HW03.ipynb)

- **Assignment 4: Generalization and Regularization: PyTorch, Autograd, Hyperparameter Tune, Overparameterization**  
  [Download Notebook](/assets/jupyter/csc578/HW04.ipynb) | [Open in Colab](https://colab.research.google.com/github/gaotx-cs/gaotx-cs.github.io/blob/main/assets/jupyter/csc578/HW04.ipynb)
